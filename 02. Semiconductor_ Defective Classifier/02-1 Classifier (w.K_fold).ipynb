{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## 차원축소 Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X1        X2        X3        X4        X5        X6        X7  \\\n",
      "X1    1.000000 -0.046819  0.073806  0.031548  0.078627 -0.083478  0.049176   \n",
      "X2   -0.046819  1.000000 -0.034894 -0.061624  0.047599  0.040274  0.005312   \n",
      "X3    0.073806 -0.034894  1.000000  0.368485  0.097051 -0.100530 -0.502848   \n",
      "X4    0.031548 -0.061624  0.368485  1.000000  0.315440 -0.550513  0.344529   \n",
      "X5    0.078627  0.047599  0.097051  0.315440  1.000000 -0.155793  0.106376   \n",
      "X6   -0.083478  0.040274 -0.100530 -0.550513 -0.155793  1.000000 -0.142037   \n",
      "X7    0.049176  0.005312 -0.502848  0.344529  0.106376 -0.142037  1.000000   \n",
      "X8   -0.077269  0.034433  0.098367 -0.095155  0.021249  0.121085 -0.181748   \n",
      "X9    0.036951  0.115223  0.156855  0.086326 -0.021352 -0.151546 -0.017449   \n",
      "X10  -0.094377  0.019367  0.008809  0.087833  0.029560  0.101066 -0.048349   \n",
      "X11   0.091001 -0.054851  0.037045  0.081226  0.008944 -0.054335  0.012160   \n",
      "X12   0.022722 -0.013238 -0.025052  0.036424  0.048184 -0.027121  0.069667   \n",
      "X13   0.086188 -0.061340  0.072458 -0.060173 -0.024349  0.061812 -0.014962   \n",
      "X14   0.041282  0.065939  0.029616 -0.007954  0.006958  0.060631  0.047298   \n",
      "X15  -0.019548  0.000642 -0.139650 -0.106910 -0.076162  0.119161 -0.000011   \n",
      "X16   0.037557  0.037662 -0.095587  0.058537 -0.040831 -0.041069  0.088896   \n",
      "X17   0.025936 -0.013492 -0.004330  0.052953  0.060228 -0.045384  0.070504   \n",
      "X18  -0.043563 -0.016005  0.038841  0.118156  0.019676 -0.137763 -0.044002   \n",
      "X19  -0.013774 -0.041521  0.038805  0.056039  0.001474 -0.043481  0.013079   \n",
      "X20  -0.057785  0.105714  0.008198 -0.115256  0.017741  0.174846 -0.155611   \n",
      "X21   0.067452 -0.109876  0.021598  0.095637 -0.035840 -0.151571  0.093820   \n",
      "X22  -0.054639  0.095010 -0.002926 -0.086220 -0.046637  0.101412 -0.123923   \n",
      "X23  -0.099035 -0.082436  0.033588  0.054959 -0.112012 -0.019441  0.068960   \n",
      "X24   0.177849 -0.050702  0.003405  0.087447  0.019664 -0.146033  0.071233   \n",
      "X25   0.141469 -0.085080  0.012219  0.107102 -0.034828 -0.182513  0.107306   \n",
      "X26   0.159862 -0.042361  0.001449  0.084198  0.036641 -0.148511  0.041710   \n",
      "X27   0.240338 -0.017554 -0.043771  0.037178  0.065635 -0.110953  0.017780   \n",
      "X28  -0.113245 -0.070252  0.132532  0.070627 -0.031363 -0.019098 -0.034222   \n",
      "X29  -0.126230  0.014121  0.145159  0.018835 -0.063621  0.033532 -0.085923   \n",
      "X30  -0.095488 -0.044629 -0.005770  0.014732 -0.106030 -0.054746  0.059348   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "X446  0.007487 -0.073653 -0.037327 -0.119630  0.092242 -0.033504 -0.097953   \n",
      "X447  0.012677  0.066534  0.008208 -0.002845 -0.073866  0.027978  0.038208   \n",
      "X448 -0.071130 -0.059339 -0.011003  0.074245  0.058258 -0.044064  0.065792   \n",
      "X449  0.043694 -0.045296  0.028234  0.026679 -0.024134 -0.005006  0.016618   \n",
      "X450 -0.002988 -0.038139  0.020371  0.048846  0.041769 -0.088088  0.009573   \n",
      "X451  0.033220 -0.038564  0.030026  0.038408 -0.025238 -0.028223  0.031020   \n",
      "X452  0.007063 -0.037305  0.025303  0.065626  0.058624 -0.089701  0.026349   \n",
      "X453  0.042662 -0.049042  0.029645  0.026964 -0.017182 -0.006912  0.013386   \n",
      "X454  0.016452 -0.020694  0.028386  0.038622  0.031085 -0.085833 -0.007833   \n",
      "X455 -0.001957 -0.051559 -0.001562 -0.042484 -0.060097  0.047549  0.004565   \n",
      "X456 -0.187882 -0.035444  0.082601 -0.012280  0.072459  0.068716 -0.145276   \n",
      "X457  0.127408  0.042592 -0.064203  0.010158 -0.029374 -0.044147  0.079603   \n",
      "X458 -0.028025  0.001866 -0.081040  0.004658  0.013100 -0.057682  0.056962   \n",
      "X459  0.121322  0.053121 -0.055300  0.015505 -0.023219 -0.048223  0.071786   \n",
      "X460 -0.015224  0.023885 -0.076734 -0.003262  0.005617 -0.053298  0.048065   \n",
      "X461  0.114737  0.046591 -0.061120  0.016940 -0.018293 -0.048303  0.075424   \n",
      "X462  0.050599  0.007425 -0.108462 -0.004723 -0.015586 -0.057988  0.097384   \n",
      "X463  0.013891  0.025647 -0.039659  0.078969  0.053553 -0.124940  0.108150   \n",
      "X464  0.026897  0.158246 -0.137008 -0.040296 -0.007165  0.036601  0.070004   \n",
      "X465  0.050058  0.150838 -0.150204 -0.049483 -0.017881  0.038541  0.077030   \n",
      "X466  0.021080  0.081334 -0.106149 -0.063344 -0.021637  0.027837  0.018915   \n",
      "X467  0.003087 -0.003852 -0.076485  0.027267 -0.005137  0.009109  0.009806   \n",
      "X468  0.016745 -0.018100  0.055246 -0.001565  0.022679 -0.026107  0.048132   \n",
      "X469  0.015428 -0.018203  0.055665 -0.003372  0.021304 -0.027239  0.044538   \n",
      "X470  0.016996 -0.017285  0.056006 -0.001466  0.023623 -0.026039  0.047759   \n",
      "X471 -0.000154  0.013084 -0.048097  0.049545  0.046439 -0.078833  0.094802   \n",
      "X472  0.021140  0.137720 -0.119191 -0.039010  0.022624  0.083009  0.087241   \n",
      "X473  0.004092  0.129259 -0.132738 -0.039752 -0.000951  0.073837  0.090302   \n",
      "X474 -0.009114  0.079061 -0.094722 -0.084236 -0.015902  0.021481  0.017935   \n",
      "Y     0.050100 -0.086191 -0.117854  0.072674  0.019963 -0.083122  0.040597   \n",
      "\n",
      "            X8        X9       X10    ...         X466      X467      X468  \\\n",
      "X1   -0.077269  0.036951 -0.094377    ...     0.021080  0.003087  0.016745   \n",
      "X2    0.034433  0.115223  0.019367    ...     0.081334 -0.003852 -0.018100   \n",
      "X3    0.098367  0.156855  0.008809    ...    -0.106149 -0.076485  0.055246   \n",
      "X4   -0.095155  0.086326  0.087833    ...    -0.063344  0.027267 -0.001565   \n",
      "X5    0.021249 -0.021352  0.029560    ...    -0.021637 -0.005137  0.022679   \n",
      "X6    0.121085 -0.151546  0.101066    ...     0.027837  0.009109 -0.026107   \n",
      "X7   -0.181748 -0.017449 -0.048349    ...     0.018915  0.009806  0.048132   \n",
      "X8    1.000000 -0.106366  0.040517    ...    -0.017342 -0.040733  0.006767   \n",
      "X9   -0.106366  1.000000 -0.000542    ...     0.011524  0.147874 -0.008388   \n",
      "X10   0.040517 -0.000542  1.000000    ...     0.027760 -0.011510 -0.068610   \n",
      "X11  -0.002090 -0.055983 -0.046115    ...     0.043017  0.093852 -0.011587   \n",
      "X12  -0.119926  0.003070 -0.103244    ...    -0.067494 -0.033916  0.036922   \n",
      "X13  -0.043635 -0.039997 -0.275145    ...    -0.119299 -0.097671  0.025763   \n",
      "X14   0.053685 -0.165556 -0.113281    ...    -0.137942 -0.078796  0.046154   \n",
      "X15   0.077632  0.009233  0.003354    ...     0.008056 -0.109669  0.050020   \n",
      "X16  -0.031500 -0.016660 -0.017153    ...     0.002642 -0.069953  0.037940   \n",
      "X17  -0.133049  0.001716 -0.104987    ...    -0.069514 -0.017813  0.029835   \n",
      "X18  -0.054525  0.089723 -0.026487    ...    -0.026015  0.019550 -0.001266   \n",
      "X19  -0.041639 -0.035711 -0.013159    ...     0.054635  0.047894 -0.058990   \n",
      "X20   0.138529 -0.012861  0.058735    ...     0.146243  0.058015 -0.017447   \n",
      "X21  -0.106612  0.054965 -0.033061    ...    -0.155736 -0.040990  0.011441   \n",
      "X22   0.114696 -0.028706  0.013580    ...     0.072514  0.019238 -0.006152   \n",
      "X23  -0.089404  0.100582  0.012194    ...    -0.102818 -0.119731  0.016106   \n",
      "X24  -0.054828  0.026604 -0.054872    ...    -0.116924 -0.025509  0.025854   \n",
      "X25  -0.088200  0.033103 -0.047298    ...    -0.149813 -0.036321  0.024485   \n",
      "X26  -0.072730  0.014699 -0.054420    ...    -0.104753 -0.018901  0.026233   \n",
      "X27  -0.101896  0.076795 -0.029904    ...    -0.065373  0.026417 -0.067196   \n",
      "X28  -0.029628  0.023259  0.007807    ...    -0.052718 -0.086788  0.099118   \n",
      "X29   0.064174  0.027340  0.010375    ...     0.014501 -0.073264  0.137368   \n",
      "X30  -0.177381  0.004204  0.034677    ...    -0.089444 -0.011746 -0.028607   \n",
      "...        ...       ...       ...    ...          ...       ...       ...   \n",
      "X446  0.009973 -0.126471 -0.041469    ...     0.010721 -0.080825 -0.038628   \n",
      "X447  0.022625  0.103169  0.033857    ...    -0.075342  0.017097 -0.020632   \n",
      "X448 -0.087894 -0.015272  0.028029    ...     0.015941 -0.049308 -0.005903   \n",
      "X449 -0.130245  0.075375 -0.036461    ...    -0.012425  0.049692 -0.004405   \n",
      "X450 -0.056133 -0.033532  0.017388    ...    -0.080408 -0.011114 -0.012712   \n",
      "X451 -0.123091  0.081809 -0.023368    ...    -0.011289  0.049772 -0.007343   \n",
      "X452 -0.056786 -0.032720  0.006543    ...    -0.082602  0.002143 -0.010598   \n",
      "X453 -0.131626  0.070336 -0.037398    ...    -0.007242  0.049273 -0.003027   \n",
      "X454 -0.034274 -0.034040  0.011603    ...    -0.092392  0.010553 -0.012223   \n",
      "X455 -0.024566 -0.016981  0.093184    ...     0.060904  0.003925  0.005040   \n",
      "X456  0.137613 -0.055476 -0.041170    ...     0.075770  0.010035 -0.024945   \n",
      "X457 -0.030084  0.079435 -0.042077    ...    -0.059669  0.104627 -0.010761   \n",
      "X458 -0.024438  0.068796 -0.067463    ...    -0.021924  0.135268 -0.038254   \n",
      "X459 -0.022322  0.078709 -0.047387    ...    -0.058893  0.097599 -0.010723   \n",
      "X460 -0.001054  0.063230 -0.074100    ...    -0.023138  0.130819 -0.034630   \n",
      "X461 -0.025621  0.073299 -0.057289    ...    -0.064702  0.098168 -0.010466   \n",
      "X462 -0.058911  0.084189 -0.047292    ...    -0.043352  0.140671 -0.029539   \n",
      "X463 -0.083161 -0.014816  0.045636    ...    -0.520889 -0.114879 -0.000223   \n",
      "X464 -0.073798  0.078333  0.102890    ...     0.439490 -0.002296  0.010202   \n",
      "X465 -0.084172  0.077671  0.119157    ...     0.468984  0.000398  0.008509   \n",
      "X466 -0.017342  0.011524  0.027760    ...     1.000000  0.076300  0.015603   \n",
      "X467 -0.040733  0.147874 -0.011510    ...     0.076300  1.000000 -0.330088   \n",
      "X468  0.006767 -0.008388 -0.068610    ...     0.015603 -0.330088  1.000000   \n",
      "X469  0.006479 -0.008059 -0.069229    ...     0.011193 -0.340223  0.998084   \n",
      "X470  0.007250 -0.008639 -0.068215    ...     0.014625 -0.334043  0.999957   \n",
      "X471 -0.044083 -0.009175 -0.003525    ...    -0.350357 -0.043568 -0.003319   \n",
      "X472 -0.063342  0.068175  0.068510    ...     0.349706 -0.003895 -0.011461   \n",
      "X473 -0.085371  0.048126  0.080473    ...     0.357923 -0.012698 -0.021127   \n",
      "X474 -0.048121 -0.015257  0.017555    ...     0.539819  0.032971 -0.023304   \n",
      "Y    -0.205588 -0.012771 -0.054252    ...    -0.029037  0.049892 -0.032687   \n",
      "\n",
      "          X469      X470      X471      X472      X473      X474         Y  \n",
      "X1    0.015428  0.016996 -0.000154  0.021140  0.004092 -0.009114  0.050100  \n",
      "X2   -0.018203 -0.017285  0.013084  0.137720  0.129259  0.079061 -0.086191  \n",
      "X3    0.055665  0.056006 -0.048097 -0.119191 -0.132738 -0.094722 -0.117854  \n",
      "X4   -0.003372 -0.001466  0.049545 -0.039010 -0.039752 -0.084236  0.072674  \n",
      "X5    0.021304  0.023623  0.046439  0.022624 -0.000951 -0.015902  0.019963  \n",
      "X6   -0.027239 -0.026039 -0.078833  0.083009  0.073837  0.021481 -0.083122  \n",
      "X7    0.044538  0.047759  0.094802  0.087241  0.090302  0.017935  0.040597  \n",
      "X8    0.006479  0.007250 -0.044083 -0.063342 -0.085371 -0.048121 -0.205588  \n",
      "X9   -0.008059 -0.008639 -0.009175  0.068175  0.048126 -0.015257 -0.012771  \n",
      "X10  -0.069229 -0.068215 -0.003525  0.068510  0.080473  0.017555 -0.054252  \n",
      "X11  -0.017734 -0.012633 -0.178567 -0.009700 -0.037305  0.108772  0.180145  \n",
      "X12   0.040858  0.037077  0.051274 -0.073752 -0.058668 -0.042280 -0.039197  \n",
      "X13   0.025246  0.026273 -0.022195 -0.060096 -0.079385 -0.082961 -0.106820  \n",
      "X14   0.048022  0.046899  0.056358 -0.018081 -0.036681 -0.123298  0.016700  \n",
      "X15   0.048101  0.050405 -0.006614  0.072792  0.100471  0.144088 -0.010076  \n",
      "X16   0.038498  0.037952 -0.041192  0.090507  0.072582  0.004351  0.195295  \n",
      "X17   0.034107  0.029933  0.052883 -0.085592 -0.074494 -0.064476 -0.038150  \n",
      "X18  -0.001879 -0.001542  0.001833 -0.025292 -0.022784  0.008053  0.133095  \n",
      "X19  -0.061516 -0.059492 -0.032835 -0.061951 -0.069009  0.003591  0.124117  \n",
      "X20  -0.017753 -0.017062  0.020375  0.128099  0.103028 -0.026603  0.015518  \n",
      "X21   0.011789  0.011219 -0.036654 -0.121719 -0.087572  0.020578  0.029680  \n",
      "X22  -0.004654 -0.006342  0.044679  0.081425  0.050078  0.001167  0.020437  \n",
      "X23   0.019316  0.016618  0.018372 -0.115149 -0.097440 -0.018959 -0.000818  \n",
      "X24   0.022848  0.026274  0.026010 -0.110785 -0.099076 -0.106516  0.168411  \n",
      "X25   0.021701  0.024569  0.007895 -0.121402 -0.097352 -0.053020  0.091560  \n",
      "X26   0.023619  0.026667  0.012975 -0.120634 -0.110945 -0.091998  0.196787  \n",
      "X27  -0.070396 -0.067434  0.118833 -0.049556 -0.043461 -0.154297  0.439797  \n",
      "X28   0.104043  0.099526 -0.109313 -0.048138 -0.049655  0.141694 -0.334744  \n",
      "X29   0.145123  0.137804 -0.097547  0.029111  0.014158  0.137837 -0.379686  \n",
      "X30  -0.026389 -0.029257 -0.055500 -0.059105 -0.038224  0.121260 -0.071000  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "X446 -0.035661 -0.038325 -0.009398 -0.068592 -0.069583 -0.030356  0.031509  \n",
      "X447 -0.021206 -0.019525  0.135583 -0.153760 -0.154331 -0.203876 -0.046049  \n",
      "X448 -0.009566 -0.005806 -0.061985  0.096113  0.121638  0.203558 -0.178739  \n",
      "X449 -0.005872 -0.004508 -0.006764  0.015543  0.029280 -0.024043  0.021695  \n",
      "X450 -0.015330 -0.012314  0.068796  0.078231  0.083361 -0.026017  0.045484  \n",
      "X451 -0.009046 -0.007320  0.000927  0.005705  0.015863 -0.040489 -0.012472  \n",
      "X452 -0.013895 -0.010281  0.081778  0.076715  0.079100 -0.049985  0.057467  \n",
      "X453 -0.004565 -0.003203 -0.014971  0.026233  0.039844 -0.009525  0.024147  \n",
      "X454 -0.013762 -0.011872  0.068760  0.040388  0.042915 -0.064157  0.083546  \n",
      "X455  0.002785  0.005045  0.035156  0.059206  0.069922  0.024173 -0.047874  \n",
      "X456 -0.025639 -0.025250 -0.112863  0.015442 -0.005973  0.138580 -0.257314  \n",
      "X457 -0.010251 -0.011324  0.038777 -0.011686 -0.030571 -0.047292 -0.073586  \n",
      "X458 -0.041651 -0.038695  0.044721 -0.006459 -0.023928 -0.020908 -0.143288  \n",
      "X459 -0.009934 -0.011253  0.035563 -0.011971 -0.028382 -0.040380 -0.080825  \n",
      "X460 -0.037933 -0.035143  0.042180 -0.015504 -0.035676 -0.018123 -0.150219  \n",
      "X461 -0.009617 -0.011027  0.032633 -0.019264 -0.038918 -0.047787 -0.061978  \n",
      "X462 -0.032317 -0.029976  0.075062 -0.006730 -0.026512 -0.059219 -0.087913  \n",
      "X463  0.004909  0.000351  0.667597  0.258954  0.220826 -0.278624  0.032205  \n",
      "X464  0.009593  0.009681  0.223910  0.790875  0.737017  0.225225  0.021328  \n",
      "X465  0.007259  0.008073  0.198735  0.769037  0.758784  0.242714  0.012279  \n",
      "X466  0.011193  0.014625 -0.350357  0.349706  0.357923  0.539819 -0.029037  \n",
      "X467 -0.340223 -0.334043 -0.043568 -0.003895 -0.012698  0.032971  0.049892  \n",
      "X468  0.998084  0.999957 -0.003319 -0.011461 -0.021127 -0.023304 -0.032687  \n",
      "X469  1.000000  0.998327 -0.001712 -0.016076 -0.026177 -0.025927 -0.035714  \n",
      "X470  0.998327  1.000000 -0.002959 -0.011837 -0.021484 -0.023750 -0.033736  \n",
      "X471 -0.001712 -0.002959  1.000000  0.216363  0.175567 -0.621348  0.084171  \n",
      "X472 -0.016076 -0.011837  0.216363  1.000000  0.965809  0.337363  0.004914  \n",
      "X473 -0.026177 -0.021484  0.175567  0.965809  1.000000  0.371324  0.004509  \n",
      "X474 -0.025927 -0.023750 -0.621348  0.337363  0.371324  1.000000 -0.115891  \n",
      "Y    -0.035714 -0.033736  0.084171  0.004914  0.004509 -0.115891  1.000000  \n",
      "\n",
      "[475 rows x 475 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"HW2.csv\")\n",
    "X, y = data.iloc[:,:474], data.iloc[:,474]\n",
    "del X[\"X69\"]\n",
    "del X[\"X189\"]\n",
    "del X[\"X192\"]\n",
    "del X[\"X288\"]\n",
    "del X[\"X293\"]\n",
    "del X[\"X389\"]\n",
    "\n",
    "'''\n",
    "df=pd.DataFrame(data).T (transpose)\n",
    "\"F로지스틱 : 선형회귀와 다르게 범주형\"#\n",
    "stepwise#\n",
    "DT CART GBM\n",
    "SVM\n",
    "RF\n",
    "NN\n",
    "관측치가 몰려있는\n",
    "PCA 축소\n",
    "SVMRFE\n",
    "'''\n",
    "corr= data.corr(method='pearson')\n",
    "print(corr)\n",
    "\n",
    "#corr.to_csv(\"HW2correlation.csv\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  [0.803921568627451, 0.803921568627451, 0.8431372549019608, 0.8431372549019608, 0.86]\n",
      "Mean of Accuracy : 0.8308\n",
      "Type 1 Error :  [0.6470588235294117, 0.7647058823529411, 0.7450980392156863, 0.7254901960784313, 0.62]\n",
      "Mean of T1Error : 0.7005\n",
      "Type 2 Error :  [0.5490196078431373, 0.43137254901960786, 0.4117647058823529, 0.43137254901960786, 0.52]\n",
      "Mean of T2Error : 0.4687\n",
      "Predict & Real :  [array([0.79310345, 0.81818182]), array([0.82857143, 0.75      ]), array([0.83333333, 0.86666667]), array([0.82857143, 0.875     ]), array([0.85714286, 0.86363636])]\n"
     ]
    }
   ],
   "source": [
    "### Logistics ###\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "logreg = LogisticRegression(solver='liblinear', multi_class='auto', max_iter=1000)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#kfold = KFold(n_splits=5)\n",
    "#rkfold = RepeatedKFold(n_splits=5, n_repeats=5, random_state =42)\n",
    "accuracy = []\n",
    "t1Error = []\n",
    "t2Error = []\n",
    "recall = []\n",
    "kf = KFold(5, random_state=0, shuffle=True)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    #print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    i=i+1\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_i = logreg.predict(X_test)\n",
    "    recall.append(recall_score(pred_i, y_test, average=None))\n",
    "    accuracy.append(accuracy_score(pred_i, y_test))\n",
    "    t1Error.append(1 - np.mean((pred_i==1)&(y_test==1)))\n",
    "    t2Error.append(1- np.mean((pred_i==-1)&(y_test==-1)))\n",
    "    \n",
    "print(\"Accuracy : \",accuracy)\n",
    "accuracy = np.array(accuracy)\n",
    "print(\"Mean of Accuracy : %.4f\" %np.mean(accuracy))\n",
    "print(\"Type 1 Error : \",t1Error)\n",
    "accuracy = np.array(t1Error)\n",
    "print(\"Mean of T1Error : %.4f\"  %np.mean(t1Error))\n",
    "print(\"Type 2 Error : \",t2Error)\n",
    "accuracy = np.array(t2Error)\n",
    "print(\"Mean of T2Error : %.4f\"  %np.mean(t2Error))\n",
    "print(\"Predict & Real : \", recall)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  [0.5294117647058824, 0.6470588235294118, 0.6274509803921569, 0.6078431372549019, 0.54]\n",
      "Mean of Accuracy : 0.5904\n",
      "Type 1 Error :  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Mean of T1Error : 1.0000\n",
      "Type 2 Error :  [0.47058823529411764, 0.3529411764705882, 0.37254901960784315, 0.3921568627450981, 0.45999999999999996]\n",
      "Mean of T2Error : 0.4096\n",
      "Predict & Real :  [array([0.52941176, 0.        ]), array([0.64705882, 0.        ]), array([0.62745098, 0.        ]), array([0.60784314, 0.        ]), array([0.54, 0.  ])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVC_clf = SVC(gamma='auto')\n",
    "#SVC_clf.fit(X, y)\n",
    "\n",
    "accuracy = []\n",
    "t1Error = []\n",
    "t2Error = []\n",
    "recall = []\n",
    "kf = KFold(5, random_state=0, shuffle=True)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    #print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    i=i+1\n",
    "    SVC_clf.fit(X_train,y_train) ## 모델 \n",
    "    pred_i = SVC_clf.predict(X_test) ## 모델\n",
    "    recall.append(recall_score(pred_i, y_test, average=None))\n",
    "    accuracy.append(accuracy_score(pred_i, y_test))\n",
    "    t1Error.append(1 - np.mean((pred_i==1)&(y_test==1)))\n",
    "    t2Error.append(1- np.mean((pred_i==-1)&(y_test==-1)))\n",
    "    \n",
    "print(\"Accuracy : \",accuracy)\n",
    "accuracy = np.array(accuracy)\n",
    "print(\"Mean of Accuracy : %.4f\" %np.mean(accuracy))\n",
    "print(\"Type 1 Error : \",t1Error)\n",
    "accuracy = np.array(t1Error)\n",
    "print(\"Mean of T1Error : %.4f\"  %np.mean(t1Error))\n",
    "print(\"Type 2 Error : \",t2Error)\n",
    "accuracy = np.array(t2Error)\n",
    "print(\"Mean of T2Error : %.4f\"  %np.mean(t2Error))\n",
    "print(\"Predict & Real : \", recall)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42564874 -0.89176611  1.12628512 ... -0.01771689  0.03644442\n",
      "   0.02534772]\n",
      " [-0.82520969 -0.40560341  0.89213553 ...  0.16937571 -0.14282832\n",
      "   0.05897878]\n",
      " [-0.44469276  0.09172787  0.77539435 ...  0.04988979  0.33787849\n",
      "  -0.26995379]\n",
      " ...\n",
      " [-0.36338682 -0.89734094 -0.67925968 ...  0.53687183 -0.29535368\n",
      "   0.10625852]\n",
      " [-0.61239784 -0.84634784 -1.16157518 ...  0.21684307 -0.15361323\n",
      "  -0.28598614]\n",
      " [-0.84626273 -0.2732561  -0.0014434  ... -0.29043616  0.04131713\n",
      "  -0.10029281]]\n"
     ]
    }
   ],
   "source": [
    "### PCA ###\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(0.9)\n",
    "pca.fit(X)\n",
    "z=pca.transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  X27                            with p-value 1.94166e-13\n",
      "Add  X70                            with p-value 7.10857e-09\n",
      "Add  X36                            with p-value 1.13706e-06\n",
      "Add  X97                            with p-value 1.81951e-05\n",
      "Add  X94                            with p-value 0.000157886\n",
      "Add  X327                           with p-value 0.000244525\n",
      "Add  X136                           with p-value 0.00137156\n",
      "Add  X235                           with p-value 0.000817562\n",
      "Add  X377                           with p-value 0.00314291\n",
      "Add  X376                           with p-value 0.00165295\n",
      "Add  X22                            with p-value 0.00442968\n",
      "Add  X161                           with p-value 0.00395181\n",
      "resulting features:\n",
      "['X27', 'X70', 'X36', 'X97', 'X94', 'X327', 'X136', 'X235', 'X377', 'X376', 'X22', 'X161']\n"
     ]
    }
   ],
   "source": [
    "### Stepwise ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "result = stepwise_selection(X, y)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          X27       X70       X36       X97       X94      X327      X136  \\\n",
      "0    0.254070  0.349287  0.611835  0.531532  0.428571  0.600829  0.250000   \n",
      "1    0.520879  0.300238  0.000000  0.518018  0.257143  0.935083  0.538462   \n",
      "2    0.171268  0.146849  0.120981  0.454955  0.314286  0.539365  0.173077   \n",
      "3    0.164898  0.153092  0.541694  0.518018  0.485714  0.544890  0.384615   \n",
      "4    0.135885  0.370690  0.603413  0.486486  0.314286  0.658149  0.384615   \n",
      "5    0.035389  0.379310  0.572181  0.531532  0.485714  0.891575  0.538462   \n",
      "6    0.449401  0.256243  0.064570  0.463964  0.457143  0.595994  0.250000   \n",
      "7    0.162777  0.285672  0.619775  0.504505  0.400000  0.597376  0.250000   \n",
      "8    0.432414  0.277051  0.064570  0.391892  0.257143  0.732735  0.250000   \n",
      "10   0.493280  0.252081  0.084134  0.509009  0.314286  0.530387  0.134615   \n",
      "11   0.062987  0.280321  0.695574  0.581081  0.600000  0.348757  0.403846   \n",
      "12   0.198166  0.325208  0.501469  0.400901  0.257143  0.520028  0.192308   \n",
      "13   0.000000  0.288347  0.668114  0.423423  0.371429  0.506906  0.192308   \n",
      "14   0.379338  0.372473  0.188490  0.486486  0.457143  0.497928  0.615385   \n",
      "15   0.307860  0.270511  0.120981  0.648649  0.542857  0.321823  0.365385   \n",
      "16   0.443032  0.339180  0.074615  0.490991  0.314286  0.495166  0.519231   \n",
      "17   0.313522  0.358502  0.188490  0.306306  0.285714  0.525552  0.615385   \n",
      "18   0.389248  0.211950  0.188490  0.396396  0.342857  0.503453  0.615385   \n",
      "19   0.282382  0.234245  0.762074  0.378378  0.371429  0.100138  0.692308   \n",
      "20   0.432414  0.198870  0.444883  0.400901  0.314286  0.482735  0.211538   \n",
      "22   0.168439  0.202140  0.118437  0.369369  0.228571  0.035221  0.192308   \n",
      "23   0.377217  0.240190  0.432601  0.328829  0.342857  0.074586  0.442308   \n",
      "24   0.343242  0.195303  0.778831  0.216216  0.314286  0.057320  0.711538   \n",
      "26   0.326261  0.191736  0.777427  0.301802  0.285714  0.122928  0.711538   \n",
      "27   0.154287  0.279429  0.501469  0.328829  0.314286  0.461326  0.750000   \n",
      "28   0.331924  0.265458  0.424003  0.342342  0.257143  0.468232  0.750000   \n",
      "30   0.397032  0.300535  0.118831  0.450450  0.342857  0.483425  0.615385   \n",
      "31   0.319892  0.131391  0.778831  0.310811  0.371429  0.088398  0.173077   \n",
      "33   0.653930  0.188466  0.843664  0.301802  0.314286  0.406768  0.500000   \n",
      "34   0.250535  0.244946  0.185814  0.373874  0.371429  0.256906  0.711538   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "215  0.537159  0.396254  1.000000  0.639640  0.485714  0.478591  0.557692   \n",
      "217  0.920032  0.258918  0.408957  0.261261  0.371429  0.901934  0.192308   \n",
      "218  0.532911  0.237812  0.542528  0.324324  0.400000  0.508287  0.442308   \n",
      "219  0.353860  0.380499  0.621880  0.378378  0.457143  0.741713  0.615385   \n",
      "220  0.705592  0.390012  0.412730  0.301802  0.371429  0.754834  0.615385   \n",
      "221  0.838643  0.299346  0.574198  0.085586  0.314286  0.252072  0.346154   \n",
      "222  0.386414  0.327883  0.601834  0.108108  0.371429  0.296271  0.442308   \n",
      "223  0.779904  0.281807  0.551388  0.509009  0.485714  0.265884  0.519231   \n",
      "225  0.561223  0.319560  0.601834  0.135135  0.342857  0.944061  0.826923   \n",
      "226  0.387121  0.335018  0.934597  0.090090  0.000000  0.718923  0.557692   \n",
      "227  0.763624  0.257134  0.582182  0.094595  0.314286  0.121547  0.538462   \n",
      "228  1.000000  0.350773  0.624337  0.198198  0.657143  0.107735  0.076923   \n",
      "229  0.778490  0.324911  0.614555  1.000000  1.000000  0.000000  0.211538   \n",
      "231  0.905879  0.313317  0.417072  0.144144  0.371429  0.202348  0.211538   \n",
      "232  0.676580  0.381391  0.460104  0.292793  0.428571  0.223066  0.692308   \n",
      "233  0.716210  0.340369  0.649033  0.382883  0.400000  0.556630  0.230769   \n",
      "234  0.740981  0.274970  0.649033  0.279279  0.514286  0.378453  0.288462   \n",
      "236  0.498943  0.565101  0.632846  0.463964  0.571429  0.405387  0.230769   \n",
      "237  0.481248  0.249108  0.580866  0.315315  0.514286  0.306630  0.365385   \n",
      "239  0.367306  0.359691  0.625258  0.207207  0.457143  0.177486  0.192308   \n",
      "240  0.954707  0.326992  0.513840  0.342342  0.400000  0.151934  0.500000   \n",
      "243  0.689318  0.308859  0.624731  0.472973  0.542857  0.280387  0.192308   \n",
      "244  0.626331  0.322830  0.615344  0.247748  0.400000  0.500691  0.519231   \n",
      "245  0.656051  0.235434  0.422687  0.243243  0.457143  0.176796  0.326923   \n",
      "247  0.631287  0.323722  0.596350  0.225225  0.400000  0.378453  0.288462   \n",
      "248  0.637656  0.352854  0.422687  0.432432  0.457143  0.511740  0.519231   \n",
      "249  0.817414  0.286266  0.552967  0.234234  0.428571  0.742403  0.230769   \n",
      "251  0.836522  0.289239  0.453218  0.207207  0.371429  0.580801  0.192308   \n",
      "252  0.831567  0.344233  0.499671  0.333333  0.457143  0.281768  0.480769   \n",
      "253  0.780611  0.219976  0.575909  0.792793  0.800000  0.917127  0.423077   \n",
      "\n",
      "         X235      X377      X376       X22      X161  \n",
      "0    0.285714  0.302358  0.233006  0.520032  0.000000  \n",
      "1    0.428571  0.453721  0.128685  0.580657  0.616766  \n",
      "2    0.071429  0.269847  0.542100  0.467611  0.233533  \n",
      "3    0.500000  0.316085  0.823923  0.526289  0.542515  \n",
      "4    0.500000  0.487983  0.693755  0.535300  0.092216  \n",
      "5    0.428571  0.212380  0.030267  0.534382  0.076647  \n",
      "6    0.142857  0.000000  0.701343  0.544950  0.405988  \n",
      "7    0.142857  0.413118  0.610950  0.641922  0.053892  \n",
      "8    0.142857  0.190228  0.774757  0.534744  0.059880  \n",
      "10   0.071429  0.366472  0.402454  0.537858  0.026347  \n",
      "11   0.285714  0.176528  0.557307  0.741481  0.059880  \n",
      "12   0.214286  0.151687  0.480607  0.563332  0.589222  \n",
      "13   0.214286  0.175002  0.392365  0.567448  0.160479  \n",
      "14   0.571429  0.165526  0.470809  0.508297  0.334132  \n",
      "15   0.428571  0.205468  0.316509  0.524176  0.608383  \n",
      "16   0.500000  0.200510  0.452085  0.348585  0.457485  \n",
      "17   0.571429  0.323378  0.405565  0.538609  0.324551  \n",
      "18   0.571429  0.429439  0.491248  0.564778  0.525749  \n",
      "19   0.642857  0.348777  0.424551  0.752994  0.057485  \n",
      "20   0.214286  0.132636  0.270018  0.750074  0.107784  \n",
      "22   0.214286  0.309505  0.453422  0.473006  0.382036  \n",
      "23   0.357143  0.095393  0.334244  0.524343  0.079042  \n",
      "24   0.642857  0.226259  0.509827  0.547898  0.328144  \n",
      "26   0.642857  0.140410  0.434872  0.431987  0.287425  \n",
      "27   0.571429  0.105337  0.432081  0.982536  0.501796  \n",
      "28   0.571429  0.234730  0.495057  0.748072  0.059880  \n",
      "30   0.500000  0.096227  0.480462  0.560162  0.281437  \n",
      "31   0.214286  0.177997  0.325115  0.856808  0.047904  \n",
      "33   0.428571  0.258845  0.376839  0.383959  0.255090  \n",
      "34   0.642857  0.125435  0.801215  0.540361  0.564072  \n",
      "..        ...       ...       ...       ...       ...  \n",
      "215  0.500000  0.184959  0.806623  0.507101  0.215569  \n",
      "217  0.214286  0.358085  0.534628  0.451555  0.494611  \n",
      "218  0.428571  0.440273  0.780398  0.453206  0.250299  \n",
      "219  0.714286  0.217393  0.247398  0.568430  0.658683  \n",
      "220  0.714286  0.209851  0.674536  0.656216  0.547305  \n",
      "221  0.285714  0.977904  0.677851  0.557409  0.405988  \n",
      "222  0.428571  0.213147  0.723062  0.554377  0.439521  \n",
      "223  0.500000  0.620536  0.673693  0.330759  0.443114  \n",
      "225  0.928571  0.200893  0.538350  0.737226  0.385629  \n",
      "226  0.500000  0.264421  0.244548  0.952974  0.532934  \n",
      "227  0.571429  0.275979  0.752137  0.692369  0.638323  \n",
      "228  0.000000  0.236264  0.722277  0.574929  0.211976  \n",
      "229  0.142857  0.244841  0.522737  0.803524  0.637126  \n",
      "231  0.142857  0.221729  0.374775  0.560412  0.389222  \n",
      "232  0.571429  0.054194  0.668169  0.613223  0.447904  \n",
      "233  0.214286  0.350759  0.447607  0.598483  0.269461  \n",
      "234  0.285714  0.570352  0.281299  0.663530  0.372455  \n",
      "236  0.285714  0.232428  0.449323  0.595035  0.780838  \n",
      "237  0.357143  0.501273  0.585277  0.428733  0.586826  \n",
      "239  0.214286  0.237556  0.626505  0.583577  0.365269  \n",
      "240  0.500000  0.261332  0.488079  0.407320  0.530539  \n",
      "243  0.214286  0.313450  0.453538  0.984955  0.671856  \n",
      "244  0.500000  0.258234  0.855207  0.175387  0.239521  \n",
      "245  0.357143  0.202552  0.394400  0.510966  0.389222  \n",
      "247  0.285714  0.552581  0.533436  0.524899  0.858683  \n",
      "248  0.500000  0.160959  0.718904  0.609440  0.428743  \n",
      "249  0.214286  0.158859  0.739111  0.464348  0.449102  \n",
      "251  0.214286  0.196586  0.686748  0.569756  0.144910  \n",
      "252  0.357143  0.232292  0.608478  0.528514  0.691018  \n",
      "253  0.285714  0.956081  0.401175  0.492056  0.786826  \n",
      "\n",
      "[204 rows x 12 columns]\n",
      "Accuracy :  [0.8235294117647058, 0.8627450980392157, 0.7647058823529411, 0.8431372549019608, 0.86]\n",
      "Mean of Accuracy : 0.8308\n",
      "Type 1 Error :  [0.6666666666666667, 0.7450980392156863, 0.7647058823529411, 0.7058823529411764, 0.64]\n",
      "Mean of T1Error : 0.7045\n",
      "Type 2 Error :  [0.5098039215686274, 0.3921568627450981, 0.47058823529411764, 0.4509803921568627, 0.5]\n",
      "Mean of T2Error : 0.4647\n",
      "Predict & Real :  [array([0.78125   , 0.89473684]), array([0.86111111, 0.86666667]), array([0.79411765, 0.70588235]), array([0.84848485, 0.83333333]), array([0.83333333, 0.9       ])]\n"
     ]
    }
   ],
   "source": [
    "SW_data=data.loc[:,['X27', 'X70', 'X36', 'X97', 'X94', 'X327', 'X136', 'X235', 'X377', 'X376', 'X22', 'X161','Y']]\n",
    "X, y = SW_data.iloc[:,:12], SW_data.iloc[:,12]\n",
    "logreg = LogisticRegression(solver='liblinear', multi_class='auto', max_iter=1000)\n",
    "#kfold = KFold(n_splits=5)\n",
    "#rkfold = RepeatedKFold(n_splits=5, n_repeats=5, random_state =42)\n",
    "accuracy = []\n",
    "t1Error = []\n",
    "t2Error = []\n",
    "recall = []\n",
    "kf = KFold(5, random_state=0, shuffle=True)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kf.split(SW_data):\n",
    "    #print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    i=i+1\n",
    "    logreg.fit(X_train,y_train)\n",
    "    pred_i = logreg.predict(X_test)\n",
    "    recall.append(recall_score(pred_i, y_test, average=None))\n",
    "    accuracy.append(accuracy_score(pred_i, y_test))\n",
    "    t1Error.append(1 - np.mean((pred_i==1)&(y_test==1)))\n",
    "    t2Error.append(1- np.mean((pred_i==-1)&(y_test==-1)))\n",
    "    \n",
    "print (X_train)\n",
    "print(\"Accuracy : \",accuracy)\n",
    "accuracy = np.array(accuracy)\n",
    "print(\"Mean of Accuracy : %.4f\" %np.mean(accuracy))\n",
    "print(\"Type 1 Error : \",t1Error)\n",
    "accuracy = np.array(t1Error)\n",
    "print(\"Mean of T1Error : %.4f\"  %np.mean(t1Error))\n",
    "print(\"Type 2 Error : \",t2Error)\n",
    "accuracy = np.array(t2Error)\n",
    "print(\"Mean of T2Error : %.4f\"  %np.mean(t2Error))\n",
    "print(\"Predict & Real : \", recall)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-161-1d03d4b0c490>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-161-1d03d4b0c490>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    print(eig_vals[i]/sum(eig_vals)\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x_std = StandardScaler().fit_transform(X)\n",
    "import numpy as np\n",
    "feature = x_std.T\n",
    "covariance_matrix = np.cov(feature)\n",
    "print(\"covariance_matrix\")\n",
    "print(covariance_matrix)\n",
    "eig_vals, eig_vecs = np.linalg.eig(covariance_matrix)\n",
    "print(\"eig_vals\")\n",
    "print(eig_vals)\n",
    "print(\"eig_vecs\")\n",
    "print(eig_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-788984593b26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m#print(train_index, test_index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "### SVM\n",
    "### Gradient\n",
    "\n",
    "'pca'\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(0.9)\n",
    "pca.fit(X)\n",
    "z=pca.transform(X)\n",
    "X=z\n",
    "\n",
    "'''stepwise'''\n",
    "#SW_data=data.loc[:,['X27', 'X70', 'X36', 'X97', 'X94', 'X327', 'X136', 'X235', 'X377', 'X376', 'X22', 'X161','Y']]\n",
    "#X, y = SW_data.iloc[:,:12], SW_data.iloc[:,12]\n",
    "\n",
    "### Random Forset\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Perceptron #, SGDClassfier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#sc=StandardScaler()\n",
    "#sc.fit(X_train) #X_train의 평균/표준편차 구함\n",
    "#X_train_std = sc.transform(X_train) #트레이닝데이터 표준화\n",
    "#X_test_std = sc.transform(X_test) #테스트 데이터 표준화\n",
    "\n",
    "ml = Perceptron(eta0 = 0.01, n_iter=40, random_state=0)\n",
    "#ml = LogisticRegression(C=1000, random_state=0)\n",
    "#ml = SVC(kernel='linear',C=1.0 ) #random_state=0\n",
    "#ml = SVC(kernel='rbf',C=10.0, gamma=0.10, random_state=0  ) #비선형 SVM c=10.0 gamma=0.10 random_state=0\n",
    "#ml = SVC(kernel='rbf',C=10.0, gamma=100.0, random_state=0 ) #비선형 SVM C=10.0 gamma=100.0 random_state=0\n",
    "#ml = linear_model.SGDClassifier(loss='perceptron')#loss='perceptrion') #확률적 경사하강법 퍼셉트론\n",
    "#ml = SGDClassifier(loss='log') #확률적 경사하강법 로지스틱\n",
    "#ml = SGDClassifier(loss='hinge')#확률적 경사하강법 SVM\n",
    "#ml = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n",
    "#ml = RandomForestClassifier(criterion='entropy', max_depth=1, n_jobs=2, random_state=1,n_estimators=100 )\n",
    "\n",
    "accuracy = []\n",
    "t1Error = []\n",
    "t2Error = []\n",
    "recall = []\n",
    "kf = KFold(5, random_state=0, shuffle=True)\n",
    "i=0\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    #print(train_index, test_index)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    i=i+1\n",
    "    ml.fit(X_train,y_train)\n",
    "    pred_i = ml.predict(X_test)\n",
    "    recall.append(recall_score(pred_i, y_test, average=None))\n",
    "    accuracy.append(accuracy_score(pred_i, y_test))\n",
    "    t1Error.append(1 - np.mean((pred_i==1)&(y_test==1)))\n",
    "    t2Error.append(1- np.mean((pred_i==-1)&(y_test==-1)))\n",
    "\n",
    "print(ml)\n",
    "print(\"Accuracy : \",accuracy)\n",
    "accuracy = np.array(accuracy)\n",
    "print(\"Mean of Accuracy : %.4f\" %np.mean(accuracy))\n",
    "print(\"Type 1 Error : \",t1Error)\n",
    "accuracy = np.array(t1Error)\n",
    "print(\"Mean of T1Error : %.4f\"  %np.mean(t1Error))\n",
    "print(\"Type 2 Error : \",t2Error)\n",
    "accuracy = np.array(t2Error)\n",
    "print(\"Mean of T2Error : %.4f\"  %np.mean(t2Error))\n",
    "print(\"Predict & Real : \", recall)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
