{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "data=pd.read_csv(\"HW1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 아무것도 조작하지 않은 그냥 데이터'''\n",
    "### dataset을 쪼개기 ###\n",
    "data_new = data.iloc[:,1:]\n",
    "train_X = np.array(scale(data_new.iloc[:,0:24])[0:20000])\n",
    "train_Y1 = np.array(data_new['Y1'][0:20000])\n",
    "train_Y2 = np.array(data_new['Y2'][0:20000]) \n",
    "test_X = np.array(scale(data_new.iloc[:,0:24])[20000:])\n",
    "test_Y1 = np.array(data_new['Y1'][20000:])\n",
    "test_Y2 = np.array(data_new['Y2'][20000:]) \n",
    "\n",
    "\n",
    "####### Y1에 대해 분석하고 싶을 때 #######\n",
    "#train_Y = train_Y1\n",
    "#test_Y = test_Y1\n",
    "\n",
    "####### Y2에 대해 분석하고 싶을 때 #######\n",
    "#train_Y = train_Y2\n",
    "#test_Y = test_Y2\n",
    "\n",
    "\n",
    "X, y = data.iloc[:,1:25], data.iloc[:,25:27]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.32837657e-01 -1.39340707e-01  8.47026215e-02  9.12057485e-02\n",
      "   1.54983259e-01  3.43426879e-01  2.74629684e-01 -1.37671617e-02\n",
      "  -1.59428203e-01 -5.22724069e-01  1.62317627e-01  7.85045657e-02\n",
      "   2.00827658e-01  8.45325702e-02 -1.01183925e-01 -1.26921121e-01\n",
      "  -1.39419542e-01 -7.25253177e-02  2.98563784e-01 -1.03410809e-01\n",
      "  -3.73860159e-01 -1.42246957e-01  6.14411370e-02 -8.36898440e-05]\n",
      " [ 1.94350202e-01 -2.80721119e-01 -3.82854778e-02  7.03759403e-02\n",
      "  -2.60652578e-01  3.18183425e-01  2.61778391e-01  1.30848572e-01\n",
      "   8.14762359e-02  2.72133475e-02  6.64746021e-02 -1.14227780e-01\n",
      "   1.74581714e-01 -7.63073685e-03  2.68162590e-03 -1.25519232e-01\n",
      "  -1.10984464e-01 -5.11519498e-01 -5.08533519e-02  3.32335032e-01\n",
      "   3.80465705e-01  1.23797892e-01 -6.28310325e-02 -3.16008701e-04]\n",
      " [ 2.01023373e-01  2.76674660e-02  3.41109047e-01  1.05844404e-01\n",
      "   3.54694689e-01 -1.32506218e-01 -2.56320757e-02 -4.17191813e-02\n",
      "  -3.24203649e-01 -1.09891299e-01  1.77059820e-01  1.34569108e-01\n",
      "   1.47627470e-01  6.86845389e-02  1.03134373e-01  1.84035646e-01\n",
      "   5.33166960e-01 -2.52514056e-01 -2.17503135e-01  2.24744448e-01\n",
      "  -2.12273614e-02  3.77892474e-02  1.07205280e-02  6.75742686e-05]\n",
      " [-2.77845812e-01  9.34221709e-02  4.64297812e-02 -8.08915070e-03\n",
      "   2.22467613e-01  1.90245722e-02  3.29090235e-03 -9.62934239e-02\n",
      "  -1.22587160e-01 -3.11129525e-01  7.32904413e-02  1.13843689e-01\n",
      "   7.71774847e-02  1.19095292e-01 -6.14047627e-02 -8.62130726e-02\n",
      "  -1.61676715e-01  3.11062653e-01 -1.75261336e-01 -4.09629912e-02\n",
      "   6.65549843e-01  2.63715775e-01 -1.53019660e-01  2.00118176e-05]\n",
      " [ 2.85864668e-01 -4.95136271e-02  1.67639810e-01  2.93179177e-02\n",
      "   4.06019772e-02  5.34383515e-03 -4.87837768e-02  1.28057963e-02\n",
      "  -1.06445726e-01  4.27248204e-02 -1.37074708e-02  1.22836715e-02\n",
      "  -6.37980388e-02  3.84096714e-02 -4.18266416e-02 -2.94468544e-01\n",
      "  -2.51748991e-01  1.47978561e-01 -8.55484136e-02  9.91034482e-02\n",
      "  -3.57992240e-01  7.33828931e-01  7.78830678e-02  5.50627108e-04]\n",
      " [ 2.75382491e-01 -6.64012908e-02  2.09381367e-01  4.35705474e-02\n",
      "   6.33421266e-02 -6.47451297e-03 -5.71292630e-02  4.55917992e-03\n",
      "  -1.46965806e-01  8.50933418e-02 -1.78298629e-02  6.89438506e-02\n",
      "   6.76209905e-02  7.29633825e-02 -6.66063705e-02 -2.94396256e-01\n",
      "  -3.81878820e-01  2.37676658e-01 -3.62534049e-01  2.39507790e-01\n",
      "  -3.01486416e-02 -5.82575361e-01 -1.85839706e-02  1.30431567e-04]\n",
      " [-1.27009131e-01 -2.80521708e-01  3.09228287e-01 -2.28817585e-01\n",
      "  -9.97121556e-02 -2.87369780e-01  2.76040996e-01 -1.00277195e-01\n",
      "   1.18333544e-01 -6.89646196e-04  2.14605425e-02 -1.02551782e-01\n",
      "  -1.37337894e-01  1.33753077e-02 -9.74965277e-03 -9.58584451e-02\n",
      "   1.91906700e-01  3.24841021e-01  3.93955234e-01  4.72603409e-01\n",
      "   5.62774949e-02 -2.33253261e-02 -2.23059144e-02 -1.41706992e-05]\n",
      " [ 1.64342163e-01 -3.04805031e-01 -5.31131134e-02  1.91779272e-01\n",
      "  -2.35764261e-01  3.91157229e-01  1.83249958e-01  1.63322877e-01\n",
      "   2.62171088e-02 -7.03920557e-03  3.02155298e-02 -1.32461638e-03\n",
      "  -3.09995943e-02  5.70646911e-02  9.97065664e-03  1.32552766e-01\n",
      "   4.08414774e-01  5.25511721e-01 -2.57439679e-01 -1.95367599e-01\n",
      "  -1.74748150e-02  1.55622973e-02 -1.21007305e-02  3.02501483e-04]\n",
      " [-1.14811486e-01 -2.61204533e-01  3.08712976e-01 -2.49589716e-01\n",
      "  -1.22897018e-01 -3.35553942e-01  3.40700724e-01 -1.37581777e-01\n",
      "   1.05763165e-01 -4.64790316e-03  6.69891148e-02 -1.52979132e-01\n",
      "   6.54296185e-02  7.14970103e-02 -4.02142656e-02  1.14675220e-01\n",
      "  -1.37818298e-01 -1.83173847e-01 -4.14974121e-01 -4.51317934e-01\n",
      "  -7.22467284e-02  3.07318980e-02  2.06368775e-02  1.24723005e-04]\n",
      " [-1.65402748e-01  1.10237581e-01  1.42774566e-01  2.87121652e-01\n",
      "   3.36923883e-01 -1.46457226e-01  1.82978082e-01  6.08680163e-01\n",
      "   4.35015972e-01  9.21463230e-02 -2.33209592e-01 -2.79184655e-02\n",
      "   2.15450167e-01  7.41415196e-02 -7.84675037e-02 -2.84739652e-02\n",
      "   2.43043510e-03  2.89154508e-02 -4.37909427e-02  4.18388847e-02\n",
      "  -3.94757997e-02  2.87990111e-02  6.98015279e-03  2.97748992e-05]\n",
      " [ 2.80407144e-01 -4.22180323e-02  1.88921793e-01  1.07762808e-01\n",
      "   4.08538804e-02 -6.63936066e-02 -2.68119699e-02  5.18865848e-02\n",
      "  -5.66792936e-02  8.05619561e-02 -2.02995371e-03 -2.02626284e-02\n",
      "  -5.05842723e-02  7.30736965e-02  1.67486725e-02 -2.15477354e-01\n",
      "   5.78369525e-02 -6.19233406e-02  3.25878854e-01 -3.72691290e-01\n",
      "   5.82972285e-02 -4.32347615e-02 -7.35205027e-01 -5.15768075e-04]\n",
      " [ 2.80288037e-01 -4.31090253e-02  1.92208890e-01  9.40396784e-02\n",
      "   5.90459737e-02 -5.21997318e-02 -1.28644485e-02  5.94433415e-02\n",
      "  -6.81648243e-02  6.21821118e-02 -6.61338645e-03 -2.25404540e-02\n",
      "  -5.57440583e-02  8.82902693e-02  3.54527796e-02 -2.29750901e-01\n",
      "   4.90240172e-02 -1.72486759e-02  3.23190595e-01 -3.73055933e-01\n",
      "   3.45077764e-01 -6.94902311e-02  6.47797117e-01 -1.70418648e-04]\n",
      " [ 1.36612126e-01  1.63937729e-02 -2.81865476e-01 -5.78493485e-01\n",
      "   1.27897994e-01  3.79749347e-02  1.13266531e-01  4.95510982e-02\n",
      "  -7.75067678e-03  2.78757984e-01  5.73781942e-02  2.86018646e-01\n",
      "   5.04099527e-01  2.36261385e-01 -1.32012975e-01 -9.98029943e-02\n",
      "   1.22321128e-01  8.37088046e-02  8.74704305e-02 -5.21221255e-02\n",
      "  -2.31308397e-02  2.86217824e-02 -6.23468285e-04 -8.81055151e-05]\n",
      " [ 2.10563189e-01  3.87555005e-02 -1.04991035e-01 -3.40325625e-01\n",
      "   3.47733173e-01  8.84378270e-02  2.02851634e-01  3.17426579e-01\n",
      "   1.27993097e-01 -1.02339032e-01  4.16791696e-01  5.88887677e-02\n",
      "  -5.15904762e-01 -2.45685302e-01  7.85119062e-02  5.68013800e-02\n",
      "  -9.77918495e-02 -1.64838463e-02 -5.74761577e-02 -1.64886796e-02\n",
      "   3.12366745e-02 -4.46746591e-02 -1.83943887e-02 -2.23900217e-06]\n",
      " [ 2.40148028e-01  3.92191201e-02 -4.11455471e-02 -3.03301462e-01\n",
      "   9.18817092e-02  6.67446656e-02  1.58761601e-01  2.97577081e-02\n",
      "  -1.31964567e-01 -3.64985758e-01 -7.84560789e-01 -9.38857767e-02\n",
      "  -1.24739775e-01  7.55332073e-03  6.66295872e-02  7.69767304e-02\n",
      "   4.16553044e-02 -4.70132349e-02 -3.90467112e-02  1.25221415e-02\n",
      "   2.72510100e-02 -1.94862695e-02 -2.01888440e-02  5.03409952e-05]\n",
      " [ 2.22271582e-01  2.33583788e-01 -1.53874622e-01  1.33581304e-01\n",
      "   6.00575597e-02 -4.26904696e-03  1.59455760e-01 -2.54441454e-01\n",
      "   2.71089129e-01 -7.71967821e-02  1.29508162e-01 -1.93189885e-01\n",
      "  -1.45182106e-01  7.40227296e-01  1.22217505e-01  1.59000753e-01\n",
      "  -8.12773921e-02  9.94463515e-03  2.36410913e-02  9.28864053e-02\n",
      "  -4.06574484e-02 -1.99866618e-02  8.17895531e-04  1.32120519e-05]\n",
      " [-2.32377751e-01 -9.29558442e-02  8.96370339e-02 -2.40266032e-01\n",
      "   5.38061077e-02  1.55483948e-01 -2.62732606e-01  3.74023188e-01\n",
      "  -3.80266754e-01  1.50186411e-01  7.54312473e-02 -5.78004545e-01\n",
      "   1.71345015e-02  2.72100406e-01  1.92086255e-01  1.02018488e-01\n",
      "  -6.74214589e-02  3.01402948e-02  3.38448067e-02  1.40243019e-02\n",
      "  -1.19103366e-02 -3.64112392e-04 -9.58439515e-03  2.82652881e-05]\n",
      " [ 1.46650926e-01  1.24036641e-01 -1.17548915e-01 -4.11563340e-02\n",
      "  -5.06175774e-01 -4.20666530e-01 -1.29015323e-01  4.42199775e-01\n",
      "  -9.17472672e-02 -4.66467931e-01  1.70274440e-01  1.40215257e-01\n",
      "   9.87842105e-02  1.18841690e-01  4.21203655e-02 -1.04815141e-02\n",
      "  -6.42429374e-03  2.46779245e-02  6.22931688e-03  2.78119765e-02\n",
      "  -1.58585605e-02 -8.64679560e-03  4.34690120e-03 -1.04252296e-06]\n",
      " [ 9.32829661e-02 -3.52680998e-01 -2.95798942e-01  7.53262927e-02\n",
      "   2.06147333e-01 -1.92082642e-01 -1.91259413e-01 -2.47848225e-02\n",
      "   1.40880288e-02 -1.25187654e-01  1.95322594e-02 -2.17378578e-01\n",
      "  -3.56972319e-02  2.35337079e-02 -4.74851663e-01 -3.32907166e-03\n",
      "   6.51142404e-02 -3.30645883e-02 -1.60893586e-02  1.39115365e-02\n",
      "   1.09492141e-02 -7.71251941e-03  5.59945211e-03  6.00276172e-01]\n",
      " [-2.00894385e-02 -4.25282346e-01 -2.05520622e-01  8.18685616e-02\n",
      "   1.73813504e-01 -1.45673616e-01 -8.07836020e-02 -4.51041895e-02\n",
      "   9.20316380e-02 -3.70258275e-02 -4.93338826e-02  1.83376021e-01\n",
      "   1.37874494e-01 -2.29034050e-02  7.63277879e-01 -1.12382858e-02\n",
      "  -1.37696956e-01  4.56357877e-02  2.39474404e-02 -1.76844364e-02\n",
      "  -1.89176589e-02  1.33802371e-02 -6.14896973e-03  1.97836122e-01]\n",
      " [ 6.71134116e-02 -3.81704934e-01 -2.81585853e-01  7.92461435e-02\n",
      "   2.04090189e-01 -1.86019255e-01 -1.68769238e-01 -3.07760458e-02\n",
      "   3.43484708e-02 -1.06502920e-01  2.48692460e-03 -1.21704960e-01\n",
      "   7.46540099e-03  1.23265001e-02 -1.72926399e-01 -5.52032961e-03\n",
      "   1.53048901e-02 -1.33988985e-02 -7.03653468e-03  6.44029924e-03\n",
      "   3.14365440e-03 -2.11899258e-03  3.18156744e-03 -7.74937730e-01]\n",
      " [ 8.13884343e-02 -9.64912843e-02  3.74549142e-01 -2.70429698e-01\n",
      "  -2.31752570e-02  2.75502459e-01 -5.62806625e-01 -4.89418801e-02\n",
      "   5.43208311e-01 -2.44850113e-01  2.07115451e-02  4.97642339e-02\n",
      "   9.31663206e-02  4.05282301e-02 -1.95781581e-02  5.92751328e-02\n",
      "   4.11930714e-02 -3.19748826e-02 -5.87833969e-03 -9.74984213e-03\n",
      "   2.42931508e-03 -6.88923572e-03  2.62475784e-03 -5.24407235e-05]\n",
      " [-2.11875200e-01 -2.91969028e-01  8.87188427e-02  1.24993557e-02\n",
      "  -6.00378831e-02  4.55335714e-02 -9.14327680e-02  1.69433681e-01\n",
      "  -1.36616541e-01  1.74447681e-01 -1.54485728e-01  5.70223922e-01\n",
      "  -4.02428730e-01  3.97225429e-01 -1.64627615e-01  1.82691155e-01\n",
      "  -9.42685269e-02 -1.73297981e-01  3.11344784e-02  2.32746053e-02\n",
      "   1.38861957e-02  1.13939688e-02  2.27743693e-03 -1.78068389e-04]\n",
      " [ 2.85090839e-01 -4.90533924e-02  1.38777567e-01  8.18345286e-02\n",
      "   2.89592844e-02 -2.22686776e-02  5.03422331e-02  1.19041712e-02\n",
      "  -8.91685216e-02  5.37100297e-02  2.44098120e-02  2.66691846e-02\n",
      "   2.42230746e-01 -1.64072208e-01 -1.12024734e-01  7.22182984e-01\n",
      "  -3.85718071e-01  1.68689686e-01  2.62955392e-01  2.29730923e-02\n",
      "   7.27050849e-02  4.71091254e-02 -1.00461590e-02 -1.43681936e-04]]\n",
      "24\n",
      "[-0.23283766  0.1943502   0.20102337 -0.27784581  0.28586467  0.27538249\n",
      " -0.12700913  0.16434216 -0.11481149 -0.16540275  0.28040714  0.28028804\n",
      "  0.13661213  0.21056319  0.24014803  0.22227158 -0.23237775  0.14665093\n",
      "  0.09328297 -0.02008944  0.06711341  0.08138843 -0.2118752   0.28509084]\n"
     ]
    }
   ],
   "source": [
    "''' PCA로 데이터를 정리해보자 '''\n",
    "\n",
    "####PCA#######\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x_std = StandardScaler().fit_transform(X)\n",
    "import numpy as np\n",
    "feature = x_std.T\n",
    "covariance_matrix = np.cov(feature)\n",
    "#print(\"covariance_matrix\")\n",
    "#print(covariance_matrix)\n",
    "eig_vals, eig_vecs = np.linalg.eig(covariance_matrix)\n",
    "#print(\"eig_vals\")\n",
    "#print(eig_vals)\n",
    "#print(\"eig_vecs\")\n",
    "print(eig_vecs)\n",
    "\n",
    "'''for i in range(0,24):\n",
    "    #print(i)\n",
    "    print(eig_vals[i]/sum(eig_vals))'''\n",
    "\n",
    "print(len(eig_vals))\n",
    "#for i in range (0,3):\n",
    "projected_X0 = X.dot(eig_vecs.T[0])\n",
    "projected_X1 = X.dot(eig_vecs.T[1])\n",
    "projected_X2 = X.dot(eig_vecs.T[2])\n",
    "projected_X3 = X.dot(eig_vecs.T[3])\n",
    "    #print(projected_X)\n",
    "#pd.DataFrame(projected_X)\n",
    "print(eig_vecs.T[0])\n",
    "\n",
    "PCA_data = pd.DataFrame(projected_X0, columns=['PC1'])\n",
    "PCA_data['PC2'] = projected_X1\n",
    "PCA_data['PC3'] = projected_X2\n",
    "PCA_data['PC4'] = projected_X3\n",
    "PCA_data['Y1'] = y.iloc[:,0]\n",
    "PCA_data['Y2'] = y.iloc[:,1]\n",
    "#print(\"PCA_data\")\n",
    "#print(PCA_data)\n",
    "#######PCA finish#####\n",
    "#### train, test로 나누기 ###\n",
    "train_X = np.array(scale(PCA_data.iloc[:,0:4])[0:20000])\n",
    "train_Y1 = np.array(PCA_data['Y1'][0:20000])\n",
    "train_Y2 = np.array(PCA_data['Y2'][0:20000])\n",
    "test_X = np.array(scale(PCA_data.iloc[:,0:4])[20000:])\n",
    "test_Y1 = np.array(PCA_data['Y1'][20000:])\n",
    "test_Y2 = np.array(PCA_data['Y2'][20000:])\n",
    "\n",
    "#print(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  X15                            with p-value 0.0\n",
      "Add  X11                            with p-value 0.0\n",
      "Add  X1                             with p-value 0.0\n",
      "Add  X19                            with p-value 0.0\n",
      "Add  X27                            with p-value 0.0\n",
      "Add  X5                             with p-value 0.0\n",
      "Add  X12                            with p-value 0.0\n",
      "Add  X20                            with p-value 4.35607e-177\n",
      "Drop X5                             with p-value 0.513881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  X17                            with p-value 7.56904e-258\n",
      "Add  X10                            with p-value 1.41423e-139\n",
      "Add  X3                             with p-value 3.72473e-240\n",
      "Add  X5                             with p-value 2.11759e-107\n",
      "Add  X6                             with p-value 0.0\n",
      "Add  X21                            with p-value 7.66959e-66\n",
      "Drop X15                            with p-value 0.465739\n",
      "Add  X9                             with p-value 2.75238e-24\n",
      "Add  X16                            with p-value 4.46831e-16\n",
      "Add  X25                            with p-value 4.35351e-17\n",
      "Add  X23                            with p-value 5.17741e-12\n",
      "Add  X22                            with p-value 3.34772e-29\n",
      "Add  X4                             with p-value 1.92812e-09\n",
      "Add  X2                             with p-value 1.45267e-11\n",
      "Add  X13                            with p-value 4.45327e-05\n",
      "Add  X15                            with p-value 0.00169248\n",
      "resulting features:\n",
      "['X11', 'X1', 'X19', 'X27', 'X12', 'X20', 'X17', 'X10', 'X3', 'X5', 'X6', 'X21', 'X9', 'X16', 'X25', 'X23', 'X22', 'X4', 'X2', 'X13', 'X15']\n"
     ]
    }
   ],
   "source": [
    "''' Stepwise Selection으로 변수를 줄여보자'''\n",
    "\n",
    "##stepwise Regression ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included\n",
    "y1,y2=y.iloc[:,0] , y.iloc[:,1]\n",
    "result = stepwise_selection(X, y2)\n",
    "\n",
    "print('resulting features:')\n",
    "print(result)\n",
    "\n",
    "sw_X= data_new.loc[:,['X11', 'X1', 'X19', 'X27', 'X12', 'X20', 'X17', 'X10', 'X3', 'X5', 'X6', 'X21', 'X9', 'X16', 'X25', 'X23', 'X22', 'X4', 'X2', 'X13', 'X15']]\n",
    "#print(sw_X)\n",
    "train_X = np.array(scale(sw_X.iloc[0:20000,:]))\n",
    "train_Y1 = np.array(data_new['Y1'][0:20000])\n",
    "train_Y2 = np.array(data_new['Y2'][0:20000]) \n",
    "test_X = np.array(scale(sw_X.iloc[20000:,:]))\n",
    "test_Y1 = np.array(data_new['Y1'][20000:])\n",
    "test_Y2 = np.array(data_new['Y2'][20000:]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VIF로 필요없는 변수를 제거해보자'''\n",
    "'''원리 : X_k가 =a1X1+a2X2+.... ak-1Xk-1 + ak+1Xk+1 + .... aiXi 꼴로 표현했을 때\n",
    "R2 score가 높다면, 다른 요소들로 충분히 X_k를 구현할 수 있다는 뜻이고, X_k가 필요없다는 결론이 나온다.\n",
    "\n",
    "보통은 1/(1-R2) 값으로 분석하며, 클 경우(10이상) 그 변수를 배제한다. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Xi\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24c0231e9649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m'''데이터가 이상해 뭐가 문제인지 확인해보자'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Mean of Xi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Variance of Xi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "'''데이터가 이상해 뭐가 문제인지 확인해보자'''\n",
    "print (\"Mean of Xi\")\n",
    "print (train_X.mean())\n",
    "print(\"Variance of Xi\")\n",
    "print (train_X.var())\n",
    "\n",
    "import pandas as pd ## 판다스 블러오기 ##\n",
    "data=pd.read_csv(\"HW1.csv\") ## 판다스에서 csv파일을 데이터프레임 타입으로 가져오기 ##\n",
    "corr= data.corr(method='pearson') ## correlation 출력 함수 by Pandas ##\n",
    "from matplotlib import pyplot as plt ## 그림그려주는 라이브러리1 matplotlib ##\n",
    "import seaborn as sns  ## 그림 그려주는 라이브러리2 ##\n",
    "plt.figure(figsize=(13,10)) ##출력할 그래프의 크기##\n",
    "sns.heatmap(corr, fmt=\"g\", linewidths=1, cmap='RdBu' ) ## Heatmap으로 표현하기 by Seaborn ##\n",
    "\n",
    ",annot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "####### Y1에 대해 분석하고 싶을 때 #######\n",
    "#train_Y = train_Y1\n",
    "#test_Y = test_Y1\n",
    "\n",
    "####### Y2에 대해 분석하고 싶을 때 #######\n",
    "train_Y = train_Y2\n",
    "test_Y = test_Y2\n",
    "\n",
    "\n",
    "Hyperparameter_list =[]\n",
    "MSETrainList=[]\n",
    "MSETestList=[]\n",
    "R2TrainList=[]\n",
    "R2TestList=[]\n",
    "    \n",
    "for i in range (1,20):\n",
    "    parameter = int(np.exp(i/2)) ## 큰 정수일때\n",
    "    #parameter = i/10 ## 0~1\n",
    "    #parameter = i ## 정수일때\n",
    "    Hyperparameter_list.append(parameter)   \n",
    "    \n",
    "    #ml = LinearRegression() ##Linear Regression ##\n",
    "    #ml = Ridge(alpha=parameter) ##Ridge## 1~20\n",
    "    #ml = Lasso(alpha=parameter) ## Lasso ## 0~1\n",
    "    #ml = DecisionTreeRegressor(max_depth=parameter) ## 1~20\n",
    "    #ml = GradientBoostingRegressor(max_depth=parameter, learning_rate=1, random_state=42)#### gradient Boosting ## #n_estimators=parameter\n",
    "    #ml = SVR(kernel='linear', C=parameter, epsilon=0.5)\n",
    "    ml = SVR(kernel='rbf', C=parameter, epsilon=0.5) #gamma = 0.01, \n",
    "    #ml = SVR(kernel='rbf', gamma = 100.0, C=parameter, epsilon=0.5)\n",
    "\n",
    "\n",
    "    ml.fit(train_X, train_Y)\n",
    "    print(ml)\n",
    "    \n",
    "    print(\"Coefficient\")\n",
    "    #print(ml.coef_)\n",
    "    print(\"intercept\")\n",
    "    #print(ml.intercept_)\n",
    "\n",
    "    ### Train ###\n",
    "    print(\"MSE(train)\")\n",
    "    y_predTrain = ml.predict(train_X)\n",
    "    lin_mse_Train = mean_squared_error(train_Y, y_predTrain)\n",
    "    lin_rmse_Train = np.sqrt(lin_mse_Train)\n",
    "    print (lin_rmse_Train)\n",
    "    MSETrainList.append(lin_rmse_Train)\n",
    "\n",
    "    print(\"R2-score(train)\")\n",
    "    y_predTrain = ml.predict(train_X)\n",
    "    print(r2_score(train_Y, y_predTrain))\n",
    "    R2TrainList.append(r2_score(train_Y, y_predTrain))\n",
    "\n",
    "    ### Test ###\n",
    "    print(\"MSE(test)\")\n",
    "    y_predTest = ml.predict(test_X)\n",
    "    lin_mseTest = mean_squared_error(test_Y, y_predTest)\n",
    "    lin_rmseTest = np.sqrt(lin_mseTest)\n",
    "    print (lin_rmseTest)\n",
    "    MSETestList.append(lin_rmseTest)     \n",
    "    \n",
    "    print(\"R2-score(test)\")\n",
    "    y_predTest = ml.predict(test_X)\n",
    "    print(r2_score(test_Y, y_predTest))\n",
    "    R2TestList.append(r2_score(test_Y, y_predTest))\n",
    "\n",
    "print(\"MaximumR2 : \",max(R2TestList)) \n",
    "    \n",
    "plt.plot(Hyperparameter_list,R2TrainList, label = \"test\")\n",
    "plt.plot(Hyperparameter_list,R2TestList, label = \"train\")\n",
    "plt.xlabel('Parameter')\n",
    "plt.ylabel('R-square')\n",
    "plt.legend(loc='center right')\n",
    "    \n",
    "\n",
    "      \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MaximumR2 : \",min(R2TestList)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=20, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Coefficient\n",
      "[-0.49202342  0.54312843  0.66392506 -0.43962413 -0.2738648   0.6964069\n",
      " -0.30583352 -0.3170255   0.29893269  0.12190383  0.34078966 -1.14269855\n",
      "  0.33975864 -0.39002992 -0.0407566  -0.32436623 -0.2076764   0.05143592\n",
      "  0.01740629  0.05353041  0.02714659 -0.00505241 -0.19491742  0.04783827]\n",
      "intercept\n",
      "0.06716052709040374\n",
      "MSE(train)\n",
      "0.18688516087799653\n",
      "MSE(test)\n",
      "0.9264117949781563\n",
      "R2-score(train)\n",
      "0.9652580788837262\n",
      "R2-score(test)\n",
      "-1.0789220345587909\n"
     ]
    }
   ],
   "source": [
    "print('''Ridge(alpha=20, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
    "Coefficient\n",
    "[-0.49202342  0.54312843  0.66392506 -0.43962413 -0.2738648   0.6964069\n",
    " -0.30583352 -0.3170255   0.29893269  0.12190383  0.34078966 -1.14269855\n",
    "  0.33975864 -0.39002992 -0.0407566  -0.32436623 -0.2076764   0.05143592\n",
    "  0.01740629  0.05353041  0.02714659 -0.00505241 -0.19491742  0.04783827]\n",
    "intercept\n",
    "0.06716052709040374\n",
    "MSE(train)\n",
    "0.18688516087799653\n",
    "MSE(test)\n",
    "0.9264117949781563\n",
    "R2-score(train)\n",
    "0.9652580788837262\n",
    "R2-score(test)\n",
    "-1.0789220345587909''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Linear Regression ##\n",
    "\n",
    "LR = LinearRegression()\n",
    "LR.fit(train_X, train_Y1)\n",
    "print(LR)\n",
    "#print(\"MSE-train\")\n",
    "#print(LR.score(train_X, train_Y1))\n",
    "#print(\"MSE-test\")\n",
    "#print(LR.score(test_X, test_Y1))\n",
    "\n",
    "print(\"Coefficient\")\n",
    "print(LR.coef_)\n",
    "print(\"intercept\")\n",
    "print(LR.intercept_)\n",
    "\n",
    "### Prediction ###\n",
    "#y_predLR = LR.predict(test_X)\n",
    "\n",
    "### MSE ###\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE(train)\")\n",
    "y_predLR = LR.predict(train_X)\n",
    "lin_mse = mean_squared_error(train_Y1, y_predLR)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print (lin_rmse)\n",
    "\n",
    "print(\"MSE(test)\")\n",
    "y_predLR = LR.predict(test_X)\n",
    "lin_mse = mean_squared_error(test_Y1, y_predLR)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print (lin_rmse)\n",
    "\n",
    "### R-square ###\n",
    "from sklearn.metrics import r2_score\n",
    "##y_predLR = LR.predict(test_X)\n",
    "\n",
    "print(\"R2-score(train)\")\n",
    "y_predLR = LR.predict(train_X)\n",
    "print(r2_score(train_Y1, y_predLR))\n",
    "\n",
    "print(\"R2-score(test)\")\n",
    "y_predLR = LR.predict(test_X)\n",
    "print(r2_score(test_Y1, y_predLR))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: 'argmin' is deprecated, use 'idxmin' instead. The behavior of 'argmin'\n",
      "will be corrected to return the positional minimum in the future.\n",
      "Use 'series.values.argmin' to get the position of the minimum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  X15                            with p-value 0.0\n",
      "Add  X11                            with p-value 0.0\n",
      "Add  X1                             with p-value 0.0\n",
      "Add  X19                            with p-value 0.0\n",
      "Add  X27                            with p-value 0.0\n",
      "Add  X5                             with p-value 0.0\n",
      "Add  X12                            with p-value 0.0\n",
      "Add  X20                            with p-value 4.35607e-177\n",
      "Drop X5                             with p-value 0.513881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\isl_005\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:35: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n",
      "will be corrected to return the positional maximum in the future.\n",
      "Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  X17                            with p-value 7.56904e-258\n",
      "Add  X10                            with p-value 1.41423e-139\n",
      "Add  X3                             with p-value 3.72473e-240\n",
      "Add  X5                             with p-value 2.11759e-107\n",
      "Add  X6                             with p-value 0.0\n",
      "Add  X21                            with p-value 7.66959e-66\n",
      "Drop X15                            with p-value 0.465739\n",
      "Add  X9                             with p-value 2.75238e-24\n",
      "Add  X16                            with p-value 4.46831e-16\n",
      "Add  X25                            with p-value 4.35351e-17\n",
      "Add  X23                            with p-value 5.17741e-12\n",
      "Add  X22                            with p-value 3.34772e-29\n",
      "Add  X4                             with p-value 1.92812e-09\n",
      "Add  X2                             with p-value 1.45267e-11\n",
      "Add  X13                            with p-value 4.45327e-05\n",
      "Add  X15                            with p-value 0.00169248\n",
      "resulting features:\n",
      "['X11', 'X1', 'X19', 'X27', 'X12', 'X20', 'X17', 'X10', 'X3', 'X5', 'X6', 'X21', 'X9', 'X16', 'X25', 'X23', 'X22', 'X4', 'X2', 'X13', 'X15']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge ##\n",
    "from sklearn.linear_model import Ridge\n",
    "Ridge_train_R2 = []\n",
    "Ridge_test_R2 = []\n",
    "Ridge_para = []\n",
    "for i in range (-50,50):\n",
    "    hyperparameter_Ridge = np.exp(i/10)\n",
    "    Ridge_para.append(hyperparameter_Ridge)\n",
    "\n",
    "    clfR = Ridge(alpha=hyperparameter_Ridge)\n",
    "    clfR.fit(train_X, train_Y1)\n",
    "    print(clfR.fit(train_X, train_Y1))\n",
    "\n",
    "    print(\"Coefficient\")\n",
    "    print(clfR.coef_)\n",
    "    print(\"intercept\")\n",
    "    print(clfR.intercept_)\n",
    "\n",
    "\n",
    "    ### MSE ###\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print(\"MSE(train)\")\n",
    "    y_predR = clfR.predict(train_X)\n",
    "    lin_mse = mean_squared_error(train_Y1, y_predR)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    print(\"MSE(test)\")\n",
    "    y_predR = clfR.predict(test_X)\n",
    "    lin_mse = mean_squared_error(test_Y1, y_predR)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    ### R-square ###\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(\"R2-score(train)\")\n",
    "    y_predR = clfR.predict(train_X)\n",
    "    print(r2_score(train_Y1, y_predR))\n",
    "    Ridge_train_R2.append(r2_score(train_Y1, y_predR))\n",
    "    print(\"R2-score(test)\")\n",
    "    y_predR = clfR.predict(test_X)\n",
    "    print(r2_score(test_Y1, y_predR))\n",
    "    Ridge_test_R2.append(r2_score(test_Y1, y_predR))\n",
    "\n",
    "    \n",
    "plt.plot(Ridge_para,Ridge_test_R2, label = \"test\")\n",
    "plt.plot(Ridge_para,Ridge_train_R2, label = \"train\")\n",
    "'''\n",
    "print(\"prediction\")\n",
    "print(y_predR)\n",
    "print(\"real_data\")\n",
    "print(test_Y1)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "print(clfR.score(train_X, train_Y1))\n",
    "print(clfR.score(test_X, test_Y1))\n",
    "###\n",
    "y_predR = clfR.predict(test_X)\n",
    "### MSE ###\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_mse = mean_squared_error(test_Y1, y_predR)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(\"MSE\")\n",
    "print (lin_rmse)\n",
    "from sklearn.metrics import r2_score\n",
    "##y_predR = clfR.predict(test_X)\n",
    "print(\"R2-score\")\n",
    "print(r2_score(test_Y1, y_predR))\n",
    "##-1.7752079132827774\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso ##\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "Lasso_depth_parameterList = []\n",
    "Lasso_R2TrainList = []\n",
    "Lasso_R2TestList = []\n",
    "Lasso_MSETrainList = []\n",
    "Lasso_MSETestList = []\n",
    "\n",
    "### 여러번 시행 hyperparameter 따라서 ###\n",
    "\n",
    "for i in range (-50,10):\n",
    "    hyperparameter_Lasso = np.exp(i/10)\n",
    "    Lasso_depth_parameterList.append(hyperparameter_Lasso)\n",
    "    #hyperparameter_Lasso = i/100\n",
    "    #print(hyperparameter)\n",
    "    clf_Lasso = Lasso(alpha=hyperparameter_Lasso)    \n",
    "    clf_Lasso.fit(train_X, train_Y1)\n",
    "\n",
    "    ##coefficient & intercept ###\n",
    "    print(Lasso(alpha=hyperparameter_Lasso) )\n",
    "    print(\"Coefficient\")\n",
    "    print(clf_Lasso.coef_)\n",
    "    print(\"intercept\")\n",
    "    print(clf_Lasso.intercept_)\n",
    "\n",
    "\n",
    "    ### MSE ###\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print(\"MSE(train)\")\n",
    "    y_pred_Lasso = clf_Lasso.predict(train_X)\n",
    "    lin_mse = mean_squared_error(train_Y1, y_pred_Lasso)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    Lasso_MSETrainList.append(lin_rmse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    print(\"MSE(test)\")\n",
    "    y_pred_Lasso = clf_Lasso.predict(test_X)\n",
    "    lin_mse = mean_squared_error(test_Y1, y_pred_Lasso)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    Lasso_MSETestList.append(lin_rmse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    ### R-square ###\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(\"R2-score(train)\")\n",
    "    y_pred_Lasso = clf_Lasso.predict(train_X)\n",
    "    print(r2_score(train_Y1, y_pred_Lasso))\n",
    "    Lasso_R2TrainList.append(r2_score(train_Y1, y_pred_Lasso))\n",
    "\n",
    "\n",
    "    print(\"R2-score(test)\")\n",
    "    y_pred_Lasso = clf_Lasso.predict(test_X)\n",
    "    print(r2_score(test_Y1, y_pred_Lasso))\n",
    "    Lasso_R2TestList.append(r2_score(test_Y1, y_pred_Lasso))    \n",
    "\n",
    "print(Lasso_R2TrainList)\n",
    "print(Lasso_R2TestList)\n",
    "print(Lasso_R2_depth_parameterList)\n",
    "    \n",
    "##x= [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "#x=[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24]\n",
    "#plt.plot(Lasso_depth_parameterList,Lasso_R2TrainList, label = \"trainR2\")\n",
    "#plt.plot(Lasso_depth_parameterList,Lasso_R2TestList, label = \"testR2\")\n",
    "plt.plot(Lasso_depth_parameterList,Lasso_MSETrainList, label = \"train_MSE\" )\n",
    "plt.plot(Lasso_depth_parameterList,Lasso_MSETestList, label = \"test_MSE\")\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='lower right')\n",
    "#plt.plot(x,DT_R2TestList, label = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decision Tree ###\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "DT_depth_parameterList = []\n",
    "DT_R2TrainList = []\n",
    "DT_R2TestList = []\n",
    "DT_MSETrainList =[]\n",
    "DT_MSETestList =[]\n",
    "for i in range(1,13):\n",
    "    depth_parameter = i\n",
    "    DT_depth_parameterList.append(depth_parameter)\n",
    "    ##model_criteria = 'entropy'\n",
    "    ##criterion='mse'\n",
    "    tree_reg = DecisionTreeRegressor(max_depth=depth_parameter)\n",
    "    print(tree_reg)\n",
    "    ##criterion='gini' criterion='entropy'\n",
    "    tree_reg.fit(train_X, train_Y1)\n",
    "\n",
    "\n",
    "    ##coefficient & intercept ###\n",
    "    '''print(\"Coefficient\")\n",
    "    print(tree_reg.coef_)\n",
    "    print(\"intercept\")\n",
    "    print(tree_reg.intercept_)'''\n",
    "\n",
    "\n",
    "\n",
    "    ### MSE ###\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print(\"MSE(train)\")\n",
    "    y_pred_Tree = tree_reg.predict(train_X)\n",
    "    lin_mse = mean_squared_error(train_Y1, y_pred_Tree)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    DT_MSETrainList.append(lin_rmse)\n",
    "    print (lin_rmse)\n",
    "    \n",
    "    print(\"MSE(test)\")\n",
    "    y_pred_Tree = tree_reg.predict(test_X)\n",
    "    lin_mse = mean_squared_error(test_Y1, y_pred_Tree)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    DT_MSETestList.append(lin_rmse)    \n",
    "    print (lin_rmse)\n",
    "\n",
    "    ### R-square ###\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(\"R2-score(train)\")\n",
    "    y_pred_Tree = tree_reg.predict(train_X)\n",
    "    print(r2_score(train_Y1, y_pred_Tree))\n",
    "    DT_R2TrainList.append(r2_score(train_Y1, y_pred_Tree))\n",
    "\n",
    "    print(\"R2-score(test)\")\n",
    "    y_pred_Tree = tree_reg.predict(test_X)\n",
    "    print(r2_score(test_Y1, y_pred_Tree))\n",
    "    DT_R2TestList.append(r2_score(test_Y1, y_pred_Tree))\n",
    "\n",
    "\n",
    "    '''\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_hat = tree_reg.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"데이터\")\n",
    "plt.plot(X_test, y_hat, color=\"cornflowerblue\", linewidth=2, label=\"예측\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(r\"$y$ & $\\hat{y}$\")\n",
    "plt.title(\"회귀 나무\")\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "   ''' \n",
    "\n",
    "#plt.plot(DT_depth_parameterList,DT_MSETrainList, label = \"train MSE\")\n",
    "#plt.plot(DT_depth_parameterList,DT_MSETestList, label = 'test MSE')\n",
    "\n",
    "plt.plot(DT_depth_parameterList,DT_R2TrainList, label = \"train R2\")\n",
    "plt.plot(DT_depth_parameterList,DT_R2TestList, label = 'test R2')\n",
    "plt.xlabel('alpha')\n",
    "#plt.ylabel('R2')\n",
    "#plt.ylabel('MSE')\n",
    "plt.legend(loc='center right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "'''from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print('Accuracy =', accuracy_score(y_test, y_pred_tree))\n",
    "'''### gradient Boosting ##\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt_estimator_list =[]\n",
    "gbrt_MSETrainList=[]\n",
    "gbrt_MSETestList=[]\n",
    "gbrt_R2TrainList=[]\n",
    "gbrt_R2TestList=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,50):\n",
    "    num_model_hyperparameter = i\n",
    "    gbrt_estimator_list.append(num_model_hyperparameter)\n",
    "    num_depth_hyperparameter = 2\n",
    "\n",
    "    gbrt = GradientBoostingRegressor(max_depth=num_depth_hyperparameter, n_estimators=num_model_hyperparameter, learning_rate=1, random_state=42)\n",
    "    print(gbrt)\n",
    "    gbrt.fit(train_X, train_Y1)\n",
    "    #print('y_pred :', gbrt.predict(test_X))\n",
    "\n",
    "\n",
    "        ### MSE ###\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print(\"MSE(train)\")\n",
    "    y_pred_gbrt = gbrt.predict(train_X)\n",
    "    lin_mse = mean_squared_error(train_Y1, y_pred_gbrt)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    gbrt_MSETrainList.append(lin_rmse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    print(\"MSE(test)\")\n",
    "    y_pred_gbrt = gbrt.predict(test_X)\n",
    "    lin_mse = mean_squared_error(test_Y1, y_pred_gbrt)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    gbrt_MSETestList.append(lin_rmse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    ### R-square ###\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(\"R2-score(train)\")\n",
    "    y_pred_gbrt = gbrt.predict(train_X)\n",
    "    print(r2_score(train_Y1, y_pred_gbrt))\n",
    "    gbrt_R2TrainList.append(r2_score(train_Y1, y_pred_gbrt))\n",
    "\n",
    "    print(\"R2-score(test)\")\n",
    "    y_pred_gbrt = gbrt.predict(test_X)\n",
    "    print(r2_score(test_Y1, y_pred_gbrt))\n",
    "    gbrt_R2TestList.append(r2_score(test_Y1, y_pred_gbrt))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "plt.plot(gbrt_estimator_list,gbrt_MSETrainList, label = \"train MSE\")\n",
    "plt.plot(gbrt_estimator_list,gbrt_MSETestList, label = 'test MSE')\n",
    "\n",
    "#plt.plot(gbrt_estimator_list,gbrt_R2TrainList, label = \"train R2\")\n",
    "#plt.plot(gbrt_estimator_list,gbrt_R2TestList, label = 'test R2')\n",
    "plt.xlabel('# of estimator')\n",
    "#plt.ylabel('R2')\n",
    "#plt.ylabel('MSE')\n",
    "plt.legend(loc='center right')    \n",
    "    \n",
    "\n",
    "'''\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(train_X, train_Y1)\n",
    "\n",
    "y2 = y - tree_reg1.predict(train_X)  # residual errors\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(train_X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(train_X)  # residual error\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(train_X, y3)\n",
    "\n",
    "X_new = np.array([[0.8]])  # new data\n",
    "y_pred = sum(tree.predict(test_X) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "print('y_pred :', y_pred)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "SVR_estimator_list =[]\n",
    "SVR_MSETrainList=[]\n",
    "SVR_MSETestList=[]\n",
    "SVR_R2TrainList=[]\n",
    "SVR_R2TestList=[]\n",
    "\n",
    "for i in range (1,15):\n",
    "    hyperparameter_SVR = int(np.exp(i/2))\n",
    "    SVR_estimator_list.append(hyperparameter_SVR)\n",
    "    clf_svr = SVR(kernel='rbf', C=hyperparameter_SVR, epsilon=0.5)\n",
    "    clf_svr.fit(train_X, train_Y1)\n",
    "\n",
    "    ##coefficient & intercept ###\n",
    "    print(clf_svr)\n",
    "    #print(\"Coefficient\")\n",
    "    #print(clf_svr.coef_)\n",
    "    #print(\"intercept\")\n",
    "    #print(clf_svr.intercept_)\n",
    "\n",
    "    ### MSE ###\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    print(\"MSE(train)\")\n",
    "    y_pred_svr = clf_svr.predict(train_X)\n",
    "    lin_mse = mean_squared_error(train_Y1, y_pred_svr)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    SVR_MSETrainList.append(lin_rmse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    print(\"MSE(test)\")\n",
    "    y_pred_svr = clf_svr.predict(test_X)\n",
    "    lin_mse = mean_squared_error(test_Y1, y_pred_svr)\n",
    "    lin_rmse = np.sqrt(lin_mse)\n",
    "    SVR_MSETestList.append(lin_rmse)\n",
    "    print (lin_rmse)\n",
    "\n",
    "    ### R-square ###\n",
    "    from sklearn.metrics import r2_score\n",
    "    print(\"R2-score(train)\")\n",
    "    y_pred_svr = clf_svr.predict(train_X)\n",
    "    SVR_R2TrainList.append(r2_score(train_Y1, y_pred_svr))\n",
    "    print(r2_score(train_Y1, y_pred_svr))\n",
    "\n",
    "    print(\"R2-score(test)\")\n",
    "    y_pred_svr = clf_svr.predict(test_X)\n",
    "    SVR_R2TestList.append(r2_score(test_Y1, y_pred_svr))\n",
    "    print(r2_score(test_Y1, y_pred_svr))\n",
    "    \n",
    "\n",
    "### Ploting ###    \n",
    "plt.plot(SVR_estimator_list,SVR_MSETrainList, label = \"train MSE\")\n",
    "plt.plot(SVR_estimator_list,SVR_MSETestList, label = 'test MSE')\n",
    "\n",
    "#plt.plot(SVR_estimator_list,SVR_R2TrainList, label = \"train R2\")\n",
    "#plt.plot(SVR_estimator_list,SVR_R2TestList, label = 'test R2')\n",
    "plt.xlabel('# of estimator')\n",
    "#plt.ylabel('R2')\n",
    "#plt.ylabel('MSE')\n",
    "plt.legend(loc='center right')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' PCA로 데이터를 정리해보자 '''\n",
    "import pandas as pd # 데이터프레임을 사용하기 위해 Pandas 모듈을 import 합니다.\n",
    "data=pd.read_csv(\"HW1.csv\") #사용하고자 하는 데이터는 dataframe 형태로 읽어옵니다.\n",
    "\n",
    "####PCA#######\n",
    "from sklearn.preprocessing import StandardScaler ##독립변수(X_i)들을 정규화 하기 위해 sklearn 모듈을 import합니다.\n",
    "x_std = StandardScaler().fit_transform(X) ## \n",
    "import numpy as np ## Covariance, lin.alg.eig 함수를 쓰기 위한 numpy 모듈을 import 합니다\n",
    "feature = x_std.T\n",
    "covariance_matrix = np.cov(feature) ## 독립변수끼리의 covariance 매트릭스를 만들어냅니다.\n",
    "print(\"covariance_matrix\")\n",
    "print(covariance_matrix)\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(covariance_matrix) ## numpy 모듈을 이용해 Eigenvalue와 Eigenvector를 구합니다\n",
    "print(\"eig_vals\")\n",
    "print(eig_vals)\n",
    "print(\"eig_vecs\")\n",
    "print(eig_vecs)\n",
    "\n",
    "### Eigen value는 변수의 갯수만큼 생기게 됩니다.\n",
    "### i번째 Eigen Value의 \"상대적 크기\"는 만들어질 PC변수_i의 설명력을 뜻하게 됩니다. (= i번째 EigenValue /전체 Eigenvalue의 합 )\n",
    "### 만일 1번째 EigenValue의 상대적 크기가 0.60라면, 곧 만들어질 PC_1은 60%의 설명력을 가진다는 의미입니다.\n",
    "### 만일 2번째 EigenValue의 상대적 크기가 0.18이라면, 만들어질 PC_2의 설명력은 18%입니다.\n",
    "### 우리가 PC1, PC2만을 채택한다면, 두개의 새로운 변수 PC1, PC2로 원본 데이터의 60+18= 78%를 설명할 수 있다는 뜻입니다.\n",
    "\n",
    "print(len(eig_vals)) ## 재조합된 PC 후보군의 수이자 =우리가 가지고 있는 변수의 갯수입니다.\n",
    "for i in range(0,24): ## 당시에는 변수의 갯수가 24개였습니다. 24개 \n",
    "    #print(i)\n",
    "    print(eig_vals[i]/sum(eig_vals))\n",
    "#위의 코드를 통해 각각 i번째 PC의 설명력을 확인하실 수 있습니다.\n",
    "\n",
    "#Eigen vector의 i번째 column은 PC_i가 원래 독립변수 X를 어떤 조합으로 만들어졌는지 확인할 수 있습니다.\n",
    "''' 각각의 PC는 어떤 조합으로 만들어졌나'''\n",
    "print(eig_vecs.T[0]) #0번째(=첫번째 PC_1) 의 조합에 들어간 X의 계수입니다.\n",
    "\n",
    "###당시에는 PC는 4개까지만 만들기로 했습니다. \n",
    "projected_X0 = X.dot(eig_vecs.T[0]) ##원래 변수(X)에서 계수를 곱해 PC값의 변수를 만드는 과정입니다.\n",
    "projected_X1 = X.dot(eig_vecs.T[1])\n",
    "projected_X2 = X.dot(eig_vecs.T[2])\n",
    "projected_X3 = X.dot(eig_vecs.T[3]) ##EigenVector중 column 0~3을 이용해 PC1~PC4를 만들어 냈습니다.\n",
    "\n",
    "PCA_data = pd.DataFrame(projected_X0, columns=['PC1']) ##PC1~PC4를 새로운 데이터프레임에 넣습니다.\n",
    "PCA_data['PC2'] = projected_X1\n",
    "PCA_data['PC3'] = projected_X2\n",
    "PCA_data['PC4'] = projected_X3\n",
    "\n",
    "print(\"PCA_data\")\n",
    "print(PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           X1        X2        X3        X4        X5        X6        X9  \\\n",
      "X1   1.000000 -0.239800 -0.375345  0.776220 -0.661140 -0.616297  0.472761   \n",
      "X2  -0.239800  1.000000  0.213486 -0.787655  0.634191  0.617126  0.005390   \n",
      "X3  -0.375345  0.213486  1.000000 -0.442097  0.804982  0.835595 -0.093614   \n",
      "X4   0.776220 -0.787655 -0.442097  1.000000 -0.860245 -0.823058  0.286247   \n",
      "X5  -0.661140  0.634191  0.804982 -0.860245  1.000000  0.992065 -0.229979   \n",
      "X6  -0.616297  0.617126  0.835595 -0.823058  0.992065  1.000000 -0.170374   \n",
      "X9   0.472761  0.005390 -0.093614  0.286247 -0.229979 -0.170374  1.000000   \n",
      "X10 -0.122346  0.960301  0.160787 -0.694629  0.548274  0.536694 -0.011877   \n",
      "X11  0.426579  0.014571 -0.067902  0.254236 -0.202530 -0.143693  0.982830   \n",
      "X12  0.431302 -0.522168 -0.099079  0.585825 -0.467510 -0.430324  0.132057   \n",
      "X13 -0.662236  0.609073  0.828888 -0.850508  0.986990  0.982860 -0.218011   \n",
      "X15 -0.649278  0.608897  0.836350 -0.842427  0.988446  0.984906 -0.213051   \n",
      "X16 -0.476171  0.241399  0.007600 -0.424097  0.280711  0.233704 -0.264244   \n",
      "X17 -0.491102  0.358707  0.443915 -0.554661  0.598752  0.550041 -0.345572   \n",
      "X18 -0.586376  0.441046  0.472903 -0.669688  0.716218  0.671105 -0.321134   \n",
      "X19 -0.682741  0.222923  0.403828 -0.576930  0.570961  0.509819 -0.707765   \n",
      "X20  0.635357 -0.410765 -0.447759  0.676933 -0.651444 -0.609891  0.467685   \n",
      "X21 -0.632063  0.216776  0.092562 -0.519079  0.358413  0.305236 -0.329779   \n",
      "X22 -0.120086  0.512538  0.014784 -0.402355  0.251612  0.240015  0.016429   \n",
      "X23  0.242950  0.387387 -0.182689 -0.089231 -0.056722 -0.037784  0.342918   \n",
      "X24 -0.030983  0.495779 -0.035183 -0.334355  0.180369  0.176220  0.100247   \n",
      "X25 -0.115987  0.216156  0.389867 -0.234759  0.426563  0.450727  0.255349   \n",
      "X26  0.697972 -0.101014 -0.441644  0.511999 -0.551515 -0.492355  0.672924   \n",
      "X27 -0.651036  0.655076  0.786917 -0.867866  0.969944  0.963525 -0.249314   \n",
      "Y1  -0.655866  0.560988  0.624839 -0.789827  0.808916  0.832153  0.022543   \n",
      "Y2  -0.776316  0.431425  0.591556 -0.783713  0.816925  0.773670 -0.526754   \n",
      "\n",
      "          X10       X11       X12    ...          X20       X21       X22  \\\n",
      "X1  -0.122346  0.426579  0.431302    ...     0.635357 -0.632063 -0.120086   \n",
      "X2   0.960301  0.014571 -0.522168    ...    -0.410765  0.216776  0.512538   \n",
      "X3   0.160787 -0.067902 -0.099079    ...    -0.447759  0.092562  0.014784   \n",
      "X4  -0.694629  0.254236  0.585825    ...     0.676933 -0.519079 -0.402355   \n",
      "X5   0.548274 -0.202530 -0.467510    ...    -0.651444  0.358413  0.251612   \n",
      "X6   0.536694 -0.143693 -0.430324    ...    -0.609891  0.305236  0.240015   \n",
      "X9  -0.011877  0.982830  0.132057    ...     0.467685 -0.329779  0.016429   \n",
      "X10  1.000000 -0.029099 -0.442276    ...    -0.340900  0.133833  0.545012   \n",
      "X11 -0.029099  1.000000  0.101134    ...     0.416387 -0.279296 -0.005696   \n",
      "X12 -0.442276  0.101134  1.000000    ...     0.358424 -0.312518 -0.344917   \n",
      "X13  0.531594 -0.189493 -0.379811    ...    -0.668853  0.368712  0.234068   \n",
      "X15  0.530637 -0.185302 -0.376864    ...    -0.659256  0.355677  0.231595   \n",
      "X16  0.100211 -0.222913 -0.500911    ...    -0.220967  0.237787  0.266786   \n",
      "X17  0.242548 -0.318716 -0.301007    ...    -0.416660  0.261667  0.245277   \n",
      "X18  0.318343 -0.282899 -0.494937    ...    -0.544796  0.380409  0.188578   \n",
      "X19  0.140811 -0.648839 -0.308902    ...    -0.831825  0.418798  0.019432   \n",
      "X20 -0.340900  0.416387  0.358424    ...     1.000000 -0.423266 -0.173936   \n",
      "X21  0.133833 -0.279296 -0.312518    ...    -0.423266  1.000000  0.023264   \n",
      "X22  0.545012 -0.005696 -0.344917    ...    -0.173936  0.023264  1.000000   \n",
      "X23  0.459008  0.300845 -0.133510    ...     0.116527 -0.246125  0.847002   \n",
      "X24  0.539226  0.072371 -0.301214    ...    -0.104967 -0.044825  0.990752   \n",
      "X25  0.174048  0.231512 -0.219106    ...     0.053879 -0.048735 -0.063153   \n",
      "X26  0.023134  0.605122  0.291411    ...     0.713215 -0.471868  0.121998   \n",
      "X27  0.572750 -0.207921 -0.439874    ...    -0.699801  0.371141  0.267228   \n",
      "Y1   0.447888  0.069259 -0.447253    ...    -0.534726  0.345364  0.291653   \n",
      "Y2   0.367413 -0.502110 -0.463985    ...    -0.681619  0.487526  0.153215   \n",
      "\n",
      "          X23       X24       X25       X26       X27        Y1        Y2  \n",
      "X1   0.242950 -0.030983 -0.115987  0.697972 -0.651036 -0.655866 -0.776316  \n",
      "X2   0.387387  0.495779  0.216156 -0.101014  0.655076  0.560988  0.431425  \n",
      "X3  -0.182689 -0.035183  0.389867 -0.441644  0.786917  0.624839  0.591556  \n",
      "X4  -0.089231 -0.334355 -0.234759  0.511999 -0.867866 -0.789827 -0.783713  \n",
      "X5  -0.056722  0.180369  0.426563 -0.551515  0.969944  0.808916  0.816925  \n",
      "X6  -0.037784  0.176220  0.450727 -0.492355  0.963525  0.832153  0.773670  \n",
      "X9   0.342918  0.100247  0.255349  0.672924 -0.249314  0.022543 -0.526754  \n",
      "X10  0.459008  0.539226  0.174048  0.023134  0.572750  0.447888  0.367413  \n",
      "X11  0.300845  0.072371  0.231512  0.605122 -0.207921  0.069259 -0.502110  \n",
      "X12 -0.133510 -0.301214 -0.219106  0.291411 -0.439874 -0.447253 -0.463985  \n",
      "X13 -0.062735  0.165241  0.389516 -0.544561  0.969254  0.813711  0.812811  \n",
      "X15 -0.063050  0.163246  0.394977 -0.543769  0.969069  0.803432  0.807892  \n",
      "X16  0.042412  0.217442  0.005506 -0.399041  0.269943  0.266839  0.306414  \n",
      "X17 -0.059471  0.174781  0.156734 -0.584435  0.582523  0.316401  0.524781  \n",
      "X18 -0.129778  0.112934  0.227616 -0.624747  0.692047  0.492626  0.640546  \n",
      "X19 -0.357327 -0.076148 -0.103422 -0.895956  0.595672  0.300365  0.769769  \n",
      "X20  0.116527 -0.104967  0.053879  0.713215 -0.699801 -0.534726 -0.681619  \n",
      "X21 -0.246125 -0.044825 -0.048735 -0.471868  0.371141  0.345364  0.487526  \n",
      "X22  0.847002  0.990752 -0.063153  0.121998  0.267228  0.291653  0.153215  \n",
      "X23  1.000000  0.911293 -0.066520  0.511951 -0.038246  0.108233 -0.225861  \n",
      "X24  0.911293  1.000000 -0.065945  0.225144  0.197174  0.253462  0.061006  \n",
      "X25 -0.066520 -0.065945  1.000000  0.019903  0.338192  0.357892  0.214152  \n",
      "X26  0.511951  0.225144  0.019903  1.000000 -0.571038 -0.315459 -0.711333  \n",
      "X27 -0.038246  0.197174  0.338192 -0.571038  1.000000  0.815093  0.775509  \n",
      "Y1   0.108233  0.253462  0.357892 -0.315459  0.815093  1.000000  0.569835  \n",
      "Y2  -0.225861  0.061006  0.214152 -0.711333  0.775509  0.569835  1.000000  \n",
      "\n",
      "[26 rows x 26 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd ## 판다스 블러오기 ##\n",
    "data=pd.read_csv(\"HW1.csv\") ## 판다스에서 csv파일을 데이터프레임 타입으로 가져오기 ##\n",
    "corr= data.corr(method='pearson') ## correlation 출력 함수 by Pandas ##\n",
    "##print(corr) \n",
    "from matplotlib import pyplot as plt ## 그림그려주는 라이브러리1 matplotlib ##\n",
    "import seaborn as sns  ## 그림 그려주는 라이브러리2 ##\n",
    "plt.figure(figsize=(13,10)) ##출력할 그래프의 크기 ##\n",
    "sns.heatmap(corr, fmt=\"g\", linewidths=1, cmap='RdBu' ) ## Heatmap으로 표현하기 by Seaborn ##\n",
    "plt.show() ##설정한 그래프 출력 ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
